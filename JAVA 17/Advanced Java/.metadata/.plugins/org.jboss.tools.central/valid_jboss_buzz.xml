<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Getting started with Debezium</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/debezium/getting-started-with-debezium/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/debezium/getting-started-with-debezium/</id><updated>2023-08-28T13:56:30Z</updated><content type="html">Debezium is a project built upon Apache Kafka and uses Kafka to stream the changes from one system to another. Once core feature of Debezium is the Change Data Capture which is able to capture data and pushes it into Kafka. In this updated tutorial we will learn how to configure Debezium and Apache Kafka ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How Ansible lint improves playbook debugging</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/28/how-ansible-lint-improves-playbook-debugging" /><author><name>Tathagata Paul, Himanshu Yadav</name></author><id>90a0a2b6-a596-4f34-92e1-fc37aae7ff8e</id><updated>2023-08-28T07:00:00Z</updated><published>2023-08-28T07:00:00Z</published><summary type="html">&lt;p&gt;Ansible lint&lt;span&gt; is a command-line tool that checks &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; playbooks for errors and suggests improvements for the code written in the playbooks. This helps the users adhere to certain standards to follow while writing the playbooks to maintain the integrity of their code.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to install Ansible lint. We will also explore use cases and how to prevent errors during execution of playbooks and save debugging time.&lt;/p&gt; &lt;h2&gt;How to install Ansible lint&lt;/h2&gt; &lt;p&gt;The easiest way to install Ansible lint is by using pip as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;python3 -m pip install –user ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) systems with a Red Hat Ansible Automation Platform subscription, we can also use &lt;code&gt;dnf&lt;/code&gt; to install Ansible lint:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can also install Ansible lint from the source repository on GitHub, but it requires &lt;strong&gt;pip&gt;=22.3.1&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pip3 install git+https://github.com/ansible/ansible-lint &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we will demonstrate two examples. One use case shows how Ansible lint throws a warning, and the other use case demonstrates a syntax error detected by Ansible lint.&lt;/p&gt; &lt;h2&gt;Example 1: Using a built-in module&lt;/h2&gt; &lt;p&gt;We will use the following code sample for an Ansible playbook (&lt;code&gt;playbook.yml&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Update apt cache hosts: all tasks: - name: Run apt-get update ansible.builtin.command: apt-get update&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To run &lt;code&gt;ansible-lint&lt;/code&gt;, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-lint playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We get the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;WARNING  Listing 2 violation(s) that are fatal command-instead-of-module: apt-get used in place of apt-get module playbook.yml:5 Task/Handler: Run apt-get update no-changed-when: Commands should not change things if nothing needs doing playbook.yml:5 Task/Handler: Run apt-get update&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice that to run an &lt;code&gt;apt-update&lt;/code&gt; on our target host, we use the &lt;code&gt;builtin.command&lt;/code&gt; instead of the &lt;code&gt;apt&lt;/code&gt; module which better serves the purpose. Ansible lint will throw warnings in this case.&lt;/p&gt; &lt;p&gt;Fixed code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Update apt cache hosts: all tasks: - name: Run apt-get update ansible.builtin.apt: update_cache: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This effectively removes the errors.&lt;/p&gt; &lt;h2&gt;Example 2: Playbook syntax error&lt;/h2&gt; &lt;p&gt;In this example, we will look at a playbook syntax error instead of a warning. Consider a playbook (&lt;code&gt;playbook.yml&lt;/code&gt;) where we set an environment variable:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Example playbook hosts: all tasks: - name: Set environment variable ansible.builtin.shell: echo $MY_ENV_VAR environment: MY_ENV_VAR: my_value&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To run &lt;code&gt;ansible-lint&lt;/code&gt;, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;ansible-lint playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We get the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;The error appears to be in '/home/tpaul/Repos/lint/playbook2.yml': line 5, column 7, but may be elsewhere in the file depending on the exact syntax problem. The offending line appears to be:   tasks:     - name: Set environment variable       ^ here&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook throws the following error and points us to the file where the error might have occurred after running Ansible lint.&lt;/p&gt; &lt;p&gt;Upon further inspection, we notice there is an indentation error in the environment line. The fixed code is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Example playbook hosts: all tasks: - name: Set environment variable ansible.builtin.shell: echo $MY_ENV_VAR environment: MY_ENV_VAR: my_value &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apart from a couple other warnings, the above code throws no other fatal errors.&lt;/p&gt; &lt;h2&gt;Ansible lint configuration file&lt;/h2&gt; &lt;p&gt;We can also customize how Ansible lint runs against playbooks according to our specific needs by using the Ansible lint configuration file.&lt;/p&gt; &lt;p&gt;Create a &lt;code&gt;.ansible-lint&lt;/code&gt; file in your working directory.&lt;/p&gt; &lt;p&gt;We can take an example of the &lt;code&gt;exclude_paths&lt;/code&gt; parameter which makes Ansible lint exclude the given paths in the configuration file.&lt;/p&gt; &lt;p&gt;Populate the configuration file with the following code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;profile: null exclude_paths: - test/playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, create a new &lt;code&gt;playbook.yml&lt;/code&gt; file inside a test directory using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir test &amp;&amp; cd test touch playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Populate the contents of the playbook with incorrect code. We will take it from the code in example 2:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Example playbook hosts: all tasks: - name: Set environment variable ansible.builtin.shell: echo $MY_ENV_VAR environment: MY_ENV_VAR: my_value &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run &lt;code&gt;ansible-lint&lt;/code&gt; in the working directory to see that no errors are thrown:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can view the full set of &lt;a href="https://ansible-lint.readthedocs.io/configuring/#ansible-lint-configuration"&gt;configuration parameters&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Continue your automation journey with Ansible&lt;/h2&gt; &lt;p&gt;You can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; the latest version of the Ansible Automation Platform at no cost. &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started&lt;/a&gt; with the Ansible Automation Platform by exploring interactive labs.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/28/how-ansible-lint-improves-playbook-debugging" title="How Ansible lint improves playbook debugging"&gt;How Ansible lint improves playbook debugging&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tathagata Paul, Himanshu Yadav</dc:creator><dc:date>2023-08-28T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.16.10.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-16-10-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-16-10-final-released/</id><updated>2023-08-26T00:00:00Z</updated><content type="html">As mentioned in previous blog posts, we encourage all our community users to upgrade to Quarkus 3. Most of the heavy lifting can be done with quarkus update but be aware that some components were updated to new major versions and that migrating might require some time and careful testing...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Eclipse Vert.x client for Apache Pinot</title><link rel="alternate" href="https://vertx.io/blog/soc-vertx-pinot-client" /><author><name>Lucifer Morningstar</name></author><id>https://vertx.io/blog/soc-vertx-pinot-client</id><updated>2023-08-25T00:00:00Z</updated><content type="html">Apache Pinot is a realtime distributed datastore for analytics workloads. To make it easier for Eclipse Vert.x users to use Apache Pinot in their applications, we introduce the Reactiverse Vert.x Client for Pinot.</content><dc:creator>Lucifer Morningstar</dc:creator></entry><entry><title>How to leverage AI to generate Apache Camel routes</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/24/how-leverage-ai-generate-apache-camel-routes" /><author><name>Andrea Tarocchi</name></author><id>2f7a1af0-e9b5-4f59-87ac-1d7f0edf3ca6</id><updated>2023-08-24T07:00:00Z</updated><published>2023-08-24T07:00:00Z</published><summary type="html">&lt;p&gt;In recent years, the integration of &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence (AI)&lt;/a&gt; in various fields has sparked new possibilities and advancements. One intriguing application of AI lies in generating domain-specific language (DSL) documents. DSLs are designed to address specific domains or problem spaces (such as enterprise integration), allowing developers to express solutions in a more natural way, removing the cognitive complexity related to boilerplate code. In this article, we will delve into the idea of leveraging AI to generate Apache Camel DSL.&lt;/p&gt; &lt;h2&gt;ChatGPT and Camel&lt;/h2&gt; &lt;p&gt;One of the primary areas where AI can be applied is in generating DSL code. By training AI models on existing Apache Camel DSL codebases, these models can learn the patterns, syntax, and semantic relationships unique to the language. With this knowledge, AI models can generate new DSL code snippets or even complete routes based on specific inputs or requirements.&lt;/p&gt; &lt;p&gt;To experiment with this idea, the following prompt was fed to ChatGPT:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Write an Apache Camel DSL that logs a message every 5 seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The answer was as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;import org.apache.camel.builder.RouteBuilder; public class LogRoute extends RouteBuilder { @Override public void configure() throws Exception { from("timer:myTimer?period=5000") .setBody().constant("Logging a message!") .to("log:myLogger"); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Encouraged by the result, I tried something more complex, a basic real world question you sometimes come across on mailing lists:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Also write an Apache Camel DSL that transforms CVS to XML.&lt;/em&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;public class CsvToXmlRoute extends RouteBuilder { @Override public void configure() throws Exception { from("file:/inputDirectory?noop=true") .unmarshal().csv() // Unmarshal CSV file into a List&lt;List&lt;String&gt;&gt; .marshal().jaxb() // Marshal List&lt;List&lt;String&gt;&gt; into XML .to("file:/outputDirectory?fileName=output.xml"); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even if the answer is assuming a lot of details, it is still more or less correct. To be fair, the prompt used was quite generic (as are most questions requesting help on a given technology most of the time) and could have been improved to get more precise answers. Nevertheless, the results were so promising that I had to see if something similar could be achieved with open components running on my laptop.&lt;/p&gt; &lt;h2&gt;PrivateGPT and Camel&lt;/h2&gt; &lt;p&gt;The space of a self-hosted large language models (LLMs) solution is quite new and rapidly evolving with new projects almost on a daily basis. I decided to experiment with &lt;a href="https://github.com/imartinez/privateGPT"&gt;PrivateGPT&lt;/a&gt; despite other seemingly promising options such as &lt;a href="https://github.com/serge-chat/serge"&gt;serge chat&lt;/a&gt; and &lt;a href="https://github.com/oobabooga/text-generation-webui"&gt;oobabooga text generation web UI&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;According to its readme, PrivateGPT is configured through an .env file. First experimentations were done with the following default settings:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;PERSIST_DIRECTORY=../db MODEL_TYPE=GPT4All MODEL_PATH=../models/ggml-gpt4all-j-v1.3-groovy.bin EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2 MODEL_N_CTX=1000 TARGET_SOURCE_CHUNKS=4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With these settings the results were not that encouraging due to producing too generic answers.&lt;/p&gt; &lt;p&gt;Fortunately, PrivateGPT makes it easy to fine-tune the training by using additional specific resources that you can provide to the model and changing it by simply changing this configuration.&lt;/p&gt; &lt;p&gt;After some experimenting with different models and adding the Apache Camel web site and examples as additional material for training, I ended up with the following configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;PERSIST_DIRECTORY=../db MODEL_TYPE=LlamaCpp MODEL_PATH=../models/WizardLM-13B-1.0.ggmlv3.q4_1.bin EMBEDDINGS_MODEL_NAME=all-mpnet-base-v2 MODEL_N_CTX=1000 TARGET_SOURCE_CHUNKS=4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using the https://huggingface.co/TheBloke/wizardLM-13B-1.0-GGML model that proved to work better at answering the following prompt:&lt;/p&gt; &lt;p&gt;DISCLAIMER: all the LLaMA models are licensed for research use only, which prevents commercial use of this model.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Write an Apache Camel DSL that logs a message every 5 seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;with this:&lt;/p&gt; &lt;p&gt;To log a message every 5 seconds using Apache Camel DSL, you can use a combination of the quartz and the log components. Here is an example of how to do this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;from("quartz://myJob?cron=0/5+0+0+1+0&amp;jobName=MyJob") .to("log:info?showAll=true"); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That is a correct snippet, using quartz component instead of the simpler timer component. Changing the prompt a little bit:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Write an Apache Camel route from timer each 5 seconds to logging.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This led to results pretty much identical to ChatGPT:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;import org.apache.camel.builder.RouteBuilder; public class MyRouteBuilder extends RouteBuilder { @Override public void configure() throws Exception { from("timer:foo?period=5000") .log("${body}"); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The time to generate the answers on my machine that has no dedicated nvidia CUDA capable GPU, took about 1.5 minutes.&lt;/p&gt; &lt;h2&gt;More experiments and conclusions&lt;/h2&gt; &lt;p&gt;With a more complex prompt, the quality of the answers decreased compared to ChatGPT, but there is room for improvement and experimentation with settings, training resources, and new models that come out daily (see &lt;a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"&gt;open LLM leaderboard&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;AI techniques, particularly Natural Language Processing (NLP), can also contribute to DSL document generation. NLP models can interpret natural language descriptions of desired DSL functionality, bridging the gap between non-technical stakeholders and developers. By analyzing and translating plain English descriptions into corresponding DSL code, AI can facilitate effective communication of domain requirements and simplify the development process.&lt;/p&gt; &lt;p&gt;Another area where AI can make an impact is in the generation of DSL documentation. AI models can analyze existing Apache Camel DSL documentation, such as official guides, user manuals, and community discussions. By understanding the structure and content of these documents, AI models can generate new and relevant documentation for DSL elements, options, and usage patterns. This automation can improve the availability and quality of documentation, benefiting both developers and end-users.&lt;/p&gt; &lt;p&gt;Similarly AI can be used to generate unit tests and usage examples, given an Apache Camel route.&lt;/p&gt; &lt;p&gt;Theoretically, AI techniques can be leveraged to perform DSL optimization and refactoring. AI models can analyze large code repositories, identifying performance bottlenecks, suggesting alternative constructs or components, and proposing architectural changes. By assisting developers in writing more efficient and maintainable DSL code, AI streamlines the development process and enhances the overall quality of DSL applications.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/24/how-leverage-ai-generate-apache-camel-routes" title="How to leverage AI to generate Apache Camel routes"&gt;How to leverage AI to generate Apache Camel routes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrea Tarocchi</dc:creator><dc:date>2023-08-24T07:00:00Z</dc:date></entry><entry><title>How to migrate a complex JBoss EAP application to OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/23/how-migrate-complex-jboss-eap-application-openshift" /><author><name>Philip Hayes</name></author><id>e3e03ffe-d779-4cc9-a6f7-dc2b76ccd0c5</id><updated>2023-08-23T07:00:00Z</updated><published>2023-08-23T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; is an ideal location to re-platform &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) workloads. We discussed this topic in previous articles, such as &lt;a href="https://developers.redhat.com/articles/2022/01/07/why-you-should-migrate-your-java-workloads-openshift"&gt;Why you should migrate your Java workloads to OpenShift.&lt;/a&gt; Here's a summary of the benefits:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: OpenShift can automatically scale JBoss EAP instances based on demand, making it more efficient and responsive to changes in traffic.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Immutability:&lt;/strong&gt; JBoss EAP applications deployed on OpenShift are built as immutable images, making it easier to promote with confidence from lower to higher environments and removing the possibility of configuration creep.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Resource allocation&lt;/strong&gt;: On OpenShift, resources can be allocated dynamically based on actual usage, allowing for better resource utilization and cost efficiency.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Deployment process&lt;/strong&gt;: OpenShift provides automated deployment and management of JBoss EAP instances, reducing the risk of errors and simplifying the deployment process.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;High availability&lt;/strong&gt;: In OpenShift, high availability is built in with automatic failover and load balancing across JBoss EAP instances.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;DevOps integration&lt;/strong&gt;: OpenShift provides a &lt;a href="https://developers.redhat.com/articles/2023/08/07/how-fully-utilize-openshift-devops"&gt;DevOps-friendly platform&lt;/a&gt; that supports &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous integration and continuous deployment (CI/CD)&lt;/a&gt; workflows, making it easier to integrate JBoss EAP applications into a larger DevOps pipeline.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the article, &lt;a href="https://developers.redhat.com/articles/2022/01/12/how-migrate-your-java-applications-red-hat-openshift"&gt;How to migrate your Java applications to Red Hat OpenShift&lt;/a&gt;, we went through the steps to migrate a simple JBoss EAP application. But what's involved with deploying a more complex real-world JBoss EAP application to OpenShift? We will describe the steps to deploy a monolith JBoss EAP application using message queues and clustering. For the purpose of this exercise, we'll use the &lt;a href="https://github.com/deewhyweb/eap-coolstore-monolith/tree/ocp"&gt;CoolStore Monolith&lt;/a&gt; application.&lt;/p&gt; &lt;p&gt;The main points of this application are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;It uses message driven beans that rely on an embedded message queue for communication.&lt;/li&gt; &lt;li aria-level="1"&gt;It uses single sign-on (SSO) for authorization.&lt;/li&gt; &lt;li aria-level="1"&gt;It requires a PostgreSQL database for persistence.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;If we look at a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/index#comparison_eap_and_xpaas_eap_image"&gt;comparison of JBoss EAP on a VM and JBoss EAP on OpenShift&lt;/a&gt;, we can see embedded messaging is not supported. Because our application requires a message queue, we need to deploy a separate instance of AMQ broker.&lt;/p&gt; &lt;p&gt;In addition to deploying our application, we will also need to deploy a PostgreSQL database, AMQ broker, and SSO.&lt;/p&gt; &lt;h2&gt;Single sign-on&lt;/h2&gt; &lt;p&gt;To deploy single sign-on in OpenShift, we'll use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.6/html/server_installation_and_configuration_guide/operator"&gt;SSO operator&lt;/a&gt;. This operator is comprehensive, allowing us to not only install an SSO cluster but also configure the required realm, client, and user objects.&lt;/p&gt; &lt;p&gt;Our sample application must have a realm, client, and user created. In addition to this, our application also needs to know the URL for the SSO instance. This URL can be configured as an environment variable when the application is deployed.&lt;/p&gt; &lt;h2&gt;AMQ broker&lt;/h2&gt; &lt;p&gt;As we mentioned earlier, embedded messaging is not supported when JBoss EAP is deployed on OpenShift. The recommended approach is to use an instance of AMQ broker to provide messaging functionality.&lt;/p&gt; &lt;p&gt;As with SSO, we can use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.4/html/deploying_amq_broker_on_openshift_container_platform/broker-operator-broker-ocp"&gt;AMQ broker operator&lt;/a&gt; to deploy an AMQ broker instance on OpenShift. Once the operator is deployed we can use custom resources to create an AMQ broker cluster and the required queues and topics. To create the AMQ broker cluster we can use the following custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: broker.amq.io/v1beta1 kind: ActiveMQArtemis metadata:   name: eap74-amq7 spec:   acceptors:     - name: my-acceptor       port: 61616       protocols: 'core'   deploymentPlan:     image: placeholder     jolokiaAgentEnabled: false     journalType: nio     managementRBACEnabled: true     messageMigration: false     persistenceEnabled: false     requireLogin: false     size: 2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once this cluster is created, we need to create the topic required by the application.&lt;/p&gt; &lt;p&gt;Looking at our app source code, we can see our application needs a topic named &lt;strong&gt;topic/orders&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@ActivationConfigProperty(propertyName = "destinationLookup", propertyValue = "topic/orders")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create this topic in our AMQ broker instance, we can use the following custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: broker.amq.io/v1beta1 kind: ActiveMQArtemisAddress metadata:   name: artemis-address-topic spec:   addressName: topic.orders   queueName: topic/orders   routingType: multicast&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We set the &lt;code&gt;routingType&lt;/code&gt; to &lt;code&gt;multicast&lt;/code&gt; because our application requires sending messages to every consumer subscribed to an address. This is described in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.0/html/using_amq_broker/addresses#configuring_publish_subscribe_messaging"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;PostgreSQL database&lt;/h2&gt; &lt;p&gt;There are multiple ways to deploy PostgreSQL databases on OpenShift. Here, we created a &lt;a href="https://github.com/deewhyweb/eap-coolstore-monolith/blob/ocp/openshift/resources/psql.yml"&gt;simple YAML script&lt;/a&gt; to create the deployment, service, and secret required to deploy the database.&lt;/p&gt; &lt;h2&gt;Application analysis&lt;/h2&gt; &lt;p&gt;During the image building process, a JBoss EAP instance is provisioned to host our application. When this JBoss EAP instance is provisioned, we can determine which layers to use to ensure our application has the capabilities required to run and remove any unnecessary capabilities to improve resource usage and reduce the attack surface.&lt;/p&gt; &lt;p&gt;You should review the supported &lt;a href="https://access.redhat.com/documentation/fr-fr/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_with_jboss_eap_for_openshift_container_platform/capability-trimming-eap-foropenshift_default#available-jboss-eap-layers_default"&gt;JBoss EAP layers&lt;/a&gt; and &lt;a href="https://docs.wildfly.org/21/Galleon_Guide.html#wildfly_galleon_layers"&gt;WildFly layers&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Looking at our application code, we can determine the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The application uses a PostgreSQL database, so it will need the PostgreSQL JDBC driver.&lt;/li&gt; &lt;li aria-level="1"&gt;The application requires web clustering, due to the presence of &lt;distributable&gt; in &lt;code&gt;web.xml&lt;/code&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;The application uses a JMS queue.&lt;/li&gt; &lt;li aria-level="1"&gt;The application includes the javax.ejb package, so EJB is required.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We can address a few of these requirements from the set of layers included with JBoss EAP 7.4:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The &lt;strong&gt;cloud-server&lt;/strong&gt; layer is the minimal layer for JBoss EAP running on OpenShift. This layer includes the health and metrics subsystems, and also includes messaging-activemq. So this meets our requirement for message queue connectivity.&lt;/li&gt; &lt;li aria-level="1"&gt;We can use the &lt;strong&gt;web-clustering&lt;/strong&gt; layer for clustering.&lt;/li&gt; &lt;li aria-level="1"&gt;We can use the &lt;strong&gt;ejb&lt;/strong&gt; layer (from the list of WildFly layers) for ejb support.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This leaves support for the PostgreSQL database. To add this, we can use the &lt;a href="https://github.com/jbossas/eap-datasources-galleon-pack"&gt;JBoss EAP datasources feature pack&lt;/a&gt;. This feature pack adds a postgresql-datasource layer that installs the required driver for PostgreSQL and sets up a JDBC DataSource to connect to the database.&lt;/p&gt; &lt;p&gt;All we need to do is define the version of the PostgreSQL JDBC driver as an environment variable: &lt;code&gt;POSTGRESQL_DRIVER_VERSION&lt;/code&gt;. We will use version 42.6.0.&lt;/p&gt; &lt;p&gt;To add these layers, environment variables, and feature packs to our application, we can specify the following to our Helm chart configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;build: uri: https://github.com/deewhyweb/eap-coolstore-monolith.git ref: ocp s2i: featurePacks: - 'org.jboss.eap:eap-datasources-galleon-pack:7.4.0.GA-redhat-00003' galleonLayers: - cloud-server - postgresql-datasource - ejb - web-clustering env: - name: POSTGRESQL_DRIVER_VERSION value: 42.6.0 deploy: enabled: false&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Application configuration&lt;/h2&gt; &lt;p&gt;Once our application is deployed to OpenShift, we will configure it to connect to our instances of PostgreSQL, AMQ broker, and SSO. All configuration is done by providing environment variables either through a ConfigMap or inline on the WildFly Server custom resource definition.&lt;/p&gt; &lt;p&gt;When we deployed our AMQ broker with the ActiveMQArtemis custom resource definition, we named the instance eap74-amq7. We must use this name in the &lt;code&gt;MQ_SERVICE_PREFIX_MAPPING&lt;/code&gt;, &lt;code&gt;EAP74_AMQ_TCP_SERVICE_HOST&lt;/code&gt;, and &lt;code&gt;AMQ_JNDI&lt;/code&gt;, as shown in the following snippet. This is documented in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/index#configuring_external_red_hat_amq_brokers"&gt;Getting Started with JBoss EAP for OpenShift Container Platform&lt;/a&gt; guide.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;  MQ_SERVICE_PREFIX_MAPPING: eap74-amq7=MQ   EAP74_AMQ_TCP_SERVICE_HOST: eap74-amq7-hdls-svc   EAP74_AMQ_TCP_SERVICE_PORT: "61616"   MQ_TOPICS: orders   AMQ_JNDI: java:/eap74-amq7/ConnectionFactory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To pull credentials from the &lt;code&gt;eap74-amq7-credentials-secret&lt;/code&gt; secret, use these configurations:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;    - name: MQ_USERNAME       valueFrom:         secretKeyRef:           key: AMQ_USER           name: eap74-amq7-credentials-secret     - name: MQ_PASSWORD       valueFrom:         secretKeyRef:           key: AMQ_PASSWORD           name: eap74-amq7-credentials-secret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To connect to the PostgreSQL database, add the following environment variables:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;  POSTGRESQL_DATABASE: postgresDB   POSTGRESQL_DATASOURCE: CoolstoreDS   POSTGRESQL_SERVICE_HOST: postgresql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To pull credentials from the PostgreSQL secret, add these configurations:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;    - name: POSTGRESQL_PASSWORD       valueFrom:         secretKeyRef:           key: database-password           name: postgresql     - name: POSTGRESQL_USER       valueFrom:         secretKeyRef:           key: database-user           name: postgresql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To connect to the SSO instance, add the following environment variable: &lt;code&gt;KEYCLOAK_URL&lt;/code&gt;. We can determine this URL once single sign-on is deployed.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;KEYCLOAK_URL: https:/&lt;&lt;red hat sso url&gt;&gt;/auth&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Clustering support&lt;/h2&gt; &lt;p&gt;Because our application requires clustering support, we must ensure our pods are deployed using a service account with permissions to view the pods in the same namespace. The default service account does not have this permission.&lt;/p&gt; &lt;p&gt;Create a service account called &lt;code&gt;coolstoresa&lt;/code&gt;. Then create a role called &lt;code&gt;pod-viewer&lt;/code&gt; and assign this role to the coolstoresa service account. We will accomplish this by applying the following YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: v1 kind: ServiceAccount metadata: name: coolstoresa --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-viewer rules: - apiGroups: [""] resources: ["pods"] verbs: ["get", "watch", "list"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-viewer subjects: - kind: ServiceAccount name: coolstoresa&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the pod starts up, you should see the following log, indicating clustering is configured correctly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;INFO Service account has sufficient permissions to view pods in kubernetes (HTTP 200). Clustering will be available.&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Testing the application by deploying on OpenShift&lt;/h2&gt; &lt;p&gt;In this section, we're going to explain the steps to deploy this application and all required components on OpenShift.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;p&gt;Before we begin, you will need to complete the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;OpenShift 4.12 with cluster admin access.&lt;/li&gt; &lt;li aria-level="1"&gt;OpenShift CLI logged into cluster with cluster admin access.&lt;/li&gt; &lt;li aria-level="1"&gt;Check out the code from &lt;a href="https://github.com/deewhyweb/eap-coolstore-monolith.git"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Run the following commands from the local directory into where the code has been cloned. Make sure to check out the ocp branch first with:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; git checkout ocp&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 1: Create a project&lt;/h3&gt; &lt;p&gt;Create a new project in OpenShift by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project coolstore&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a new project called coolstore.&lt;/p&gt; &lt;h3&gt;Step 2: Deploy the operators&lt;/h3&gt; &lt;p&gt;Deploy the SSO, AMQ broker, and JBoss EAP operators by creating subscription custom resources.&lt;/p&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f./openshift/operators&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will deploy the necessary operators to the cluster. Wait for the operators to be deployed by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get csv -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait until their ClusterServiceVersion PHASE is set to "Succeeded".&lt;/p&gt; &lt;h3&gt;Step 3: Deploy resources&lt;/h3&gt; &lt;p&gt;Now that the operators have been deployed, we can create the coolstoresa service account and deploy the SSO, AMQ broker, and PostgreSQL instances.&lt;/p&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f./openshift/resources&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run the following command to get the route for the SSO instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get route keycloak&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It may take a few attempts to create the route. Once you have the route, update the &lt;code&gt;KEYCLOAK_URL&lt;/code&gt; value in &lt;code&gt;helm.yaml&lt;/code&gt; with the correct route for SSO. Make sure to prepend &lt;code&gt;https://&lt;/code&gt; and append &lt;code&gt;/auth&lt;/code&gt; to this URL. The value should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;  - name: KEYCLOAK_URL     value: https://keycloak-coolstore.apps.openshift.domainname.com/auth&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 4: Build the application image&lt;/h3&gt; &lt;p&gt;To build the application image, navigate to the developer UI and click on &lt;strong&gt;+Add&lt;/strong&gt;, then &lt;strong&gt;Helm Chart.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Select the Eap74 Helm chart.&lt;/p&gt; &lt;p&gt;Paste the contents of &lt;code&gt;openshift/helm.yml&lt;/code&gt; as the config.&lt;/p&gt; &lt;p&gt;Wait for the application image to be built by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get build -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application image is built when the eap74-2 build is complete.&lt;/p&gt; &lt;h3&gt;Step 5: Deploy the application&lt;/h3&gt; &lt;p&gt;We can deploy the application by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f./openshift/app&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 6: Testing the application&lt;/h3&gt; &lt;p&gt;Once the application is running, you should see a topology similar to Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topology_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topology_0.jpg?itok=sSOvEE7M" width="600" height="389" alt="A screenshot of the OpenShift topology view, showing running applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The OpenShift topology view of the deployed application, showing running applications.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You should be able to access this via the external route. From the application, click on &lt;strong&gt;Sign in&lt;/strong&gt; in the top right-hand corner. This will take you to the single sign-on login page. Log in with the credentials user1 / pass.&lt;/p&gt; &lt;p&gt;Now, you should be able to add products to your cart and complete the checkout process.&lt;/p&gt; &lt;h3&gt;Step 7: Testing clustering&lt;/h3&gt; &lt;p&gt;If we scale up our application, we should see additional members forming a cluster. To scale up the application, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc scale WildFlyServer eap74 --replicas=2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While the second instance starts, monitor the logs of the existing pod. You should see the addition of a node to the cluster similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;10:46:45,408 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 63) ISPN000078: Starting JGroups channel ee 10:46:45,501 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 63) ISPN000094: Received new cluster view for channel ee: [eap74-0] (2) [eap74-0, eap74-1] 10:46:45,507 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 63) ISPN000079: Channel ee local address is eap74-1, physical addresses are [10.130.0.67:7600]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;JBoss EAP migrated to OpenShift&lt;/h2&gt; &lt;p&gt;In this article, we migrated a real-world monolith JBoss EAP application to OpenShift. The application required a PostgreSQL database, message queue, and single sign-on. We used &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; operators to deploy and configure AMQ broker and single sign-on.&lt;/p&gt; &lt;p&gt;We also performed analysis on the application to determine which layers to use and concluded the requirement of the following layers:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;cloud-server&lt;/li&gt; &lt;li&gt;ejb&lt;/li&gt; &lt;li&gt;web-clustering&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition to these layers, we also used the postgresql-datasource from the JBoss EAP-datasources-galleon-pack to provide PostgreSQL database drivers and configuration.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/23/how-migrate-complex-jboss-eap-application-openshift" title="How to migrate a complex JBoss EAP application to OpenShift"&gt;How to migrate a complex JBoss EAP application to OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Philip Hayes</dc:creator><dc:date>2023-08-23T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.3.0 released - OpenTelemetry improvements, Reactive Messaging Pulsar extension</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-3-0-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-3-0-released/</id><updated>2023-08-23T00:00:00Z</updated><content type="html">It is our pleasure to announce the release of Quarkus 3.3.0. The first thing you will notice in this release is that we dropped the .Final suffix. This suffix was introduced to make sure final releases were sorted properly compared to alphas, beta and candidate releases, at a time where...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Migration toolkit for applications 6.2: Agile Java modernization</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/22/migration-toolkit-applications-62-agile-java-modernization" /><author><name>Yashwanth Maheshwaram</name></author><id>a1f6500c-f583-4d1c-8cc7-9e2255a829fa</id><updated>2023-08-22T17:30:00Z</updated><published>2023-08-22T17:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/mta/overview"&gt;Red Hat's migration toolkit for applications 6.2&lt;/a&gt; takes &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; application modernization a step further with new integrations for Jira, migration waves management, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; monitoring.&lt;/p&gt; &lt;p&gt;With these integrations, the migration toolkit for applications can now help organizations create a backlog of work directly on Jira so that your engineering teams can pick up tickets. The support for migration waves enables you to process the backlog in batches and then plan for the whole application modernization at once. You can also make migration waves available directly on Jira. &lt;/p&gt; &lt;p&gt;Read on for a complete list of new features in the migration toolkit for applications 6.2, available with a Red Hat OpenShift subscription.&lt;/p&gt; &lt;h2&gt;Jira integration&lt;/h2&gt; &lt;p&gt;The integration of the migration toolkit for applications with Jira allows you to track and manage the whole migration process. To introduce changes to the applications in the portfolio, you can create issues in Jira and assign them to developers.&lt;/p&gt; &lt;p&gt;For more information, see &lt;a href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_applications/6.2/html/user_interface_guide/creating-configuring-jira-connection#mta-web-create-config-jira-connection_user-interface-guide"&gt;Creating and configuring a Jira connection&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Migration waves management&lt;/h2&gt; &lt;p&gt;A migration wave is a small collection of workloads that deliver business value. Migration waves group applications to be migrated on a specified schedule.&lt;/p&gt; &lt;p&gt;In addition, a migration wave enables you to export a list of the wave’s applications to the Jira issue management system. This automatically creates a separate Jira issue for each application of the migration wave for tracking.&lt;/p&gt; &lt;p&gt;For more information, see &lt;a href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_applications/6.2/html/user_interface_guide/working-with-applications-in-the-ui#mta-web-creating-migration-waves_user-interface-guide"&gt;Creating migration waves&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_applications/6.2/html/user_interface_guide/working-with-applications-in-the-ui#mta-web-creating-jira-issues-for-migration-wave_user-interface-guide"&gt;Creating Jira issues for a migration wave&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;OpenShift monitoring integration&lt;/h2&gt; &lt;p&gt;The migration toolkit for applications integrates with Red Hat OpenShift's monitoring stack. This allows Red Hat to collect data that will display the usage of the migration toolkit in the field by customers beyond installation.&lt;/p&gt; &lt;h2&gt;Modernize securely at scale&lt;/h2&gt; &lt;p&gt;Red Hat's migration toolkit for applications provides a simpler, faster way for development teams modernize and migrate applications to Red Hat OpenShift, providing tools and best practices to accelerate your journey to Kubernetes. You can get access to the migration toolkit for applications with an OpenShift subscription. &lt;a href="https://developers.redhat.com/products/mta/getting-started"&gt;Get started today.&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/22/migration-toolkit-applications-62-agile-java-modernization" title="Migration toolkit for applications 6.2: Agile Java modernization"&gt;Migration toolkit for applications 6.2: Agile Java modernization&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2023-08-22T17:30:00Z</dc:date></entry><entry><title type="html">How to create Jobs in Kubernetes</title><link rel="alternate" href="https://www.mastertheboss.com/soa-cloud/openshift/how-to-create-jobs-in-kubernetes/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/soa-cloud/openshift/how-to-create-jobs-in-kubernetes/</id><updated>2023-08-22T12:52:25Z</updated><content type="html">This article discusses how to automate Tasks in Kubernetes and OpenShift using Jobs and Cron Jobs. We will show some example on how to create and manage them. Then, we will discuss the best practices about using Jobs in Kubernetes. In a Kubernetes environment, you can use Jobs to automate tasks that need to run ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Enhance Ansible development experience with Lightspeed</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/22/enhance-ansible-development-experience-lightspeed" /><author><name>Nagesh Rathod</name></author><id>cf8a9c07-f235-4911-813c-973e23674812</id><updated>2023-08-22T07:00:00Z</updated><published>2023-08-22T07:00:00Z</published><summary type="html">&lt;p&gt;Ansible Lightspeed is a generative AI tool that provides an efficient way for developers to create &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Ansible&lt;/a&gt; content and &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; tasks for Ansible playbooks. In this article, we will explore how to install and use Ansible Lightspeed in Visual Studio Code. This makes it easy to create Ansible content; you can simply type in plain English descriptions and Ansible Lightspeed will generate the code recommendations for you.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before proceeding through this tutorial, make sure you have installed the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Visual Studio Code (VS Code)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; 3.9+&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/ansible/"&gt;Ansible Automation Platform&lt;/a&gt; 2.x (see &lt;a href="https://developers.redhat.com/blog/2023/08/04/how-install-ansible-automation-platform-24-rhel-91"&gt;How to install Ansible Automation Platform 2.4 on RHEL 9.1&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;1. Install Ansible extension in VS Code&lt;/h2&gt; &lt;p&gt;The Ansible extension offers intelligent code completion, syntax highlighting, and error checking for YAML and Ansible-specific files. The extension allows users to execute Ansible tasks and playbooks directly from the editor and view the real-time output. With built-in support for Ansible Vault and Galaxy, developers can efficiently manage secrets and access a vast library of community-contributed roles.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt; &lt;p&gt;In VS Code, click the extension icon in the left menu. In the search field, type &lt;code&gt;Ansible&lt;/code&gt; to bring up the Ansible extension by Red Hat.&lt;/p&gt; &lt;/li&gt; &lt;li aria-level="1"&gt;Click the &lt;strong&gt;Install &lt;/strong&gt;button. &lt;/li&gt; &lt;li aria-level="1"&gt;After installing the Ansible extension in the integrated development environment (IDE), click on the little gear icon beside the Install button and select the &lt;strong&gt;Extension Setting &lt;/strong&gt;option, as illustrated in Figure 1.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture1.png?itok=hdan4ffL" width="600" height="351" alt="Vs code extention" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Installing the Ansible extension in Visual Studio Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;2. Enable Ansible Lightspeed in VS Code&lt;/h2&gt; &lt;p&gt;You will get the following settings page of Ansible Extension, from which you can make changes to execution environment image, container registry, Ansible lint, and many more. This article only covers Lightspeed, so we will just enable the &lt;strong&gt;Ansible Lightspeed&lt;/strong&gt; option and&lt;strong&gt; Ansible Lightspeed with Watson Code Assistant inline suggestions&lt;/strong&gt;. Please refer to Figure 2 to set the configuration.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture2.png?itok=zFCHJwNZ" width="600" height="338" alt="enable it" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Enable Lightspeed and Watson Code Assistant.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you get the error "Ansible-lint is missing," use the following commands to install it. There are two methods to install: &lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;dnf&lt;/code&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Install using &lt;code&gt;pip&lt;/code&gt;: &lt;pre&gt; &lt;code class="language-bash"&gt;pip3 install ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;ol start="2"&gt;&lt;li aria-level="1"&gt;Install with the &lt;code&gt;dnf&lt;/code&gt; package manager: &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;3. Authenticate Lightspeed with GitHub&lt;/h2&gt; &lt;p&gt;Click on the Ansible extension icon on the left bar. You will see the &lt;strong&gt;Connect&lt;/strong&gt; button, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture3.png?itok=Wmb7FBir" width="600" height="690" alt="connect" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Connect Ansible to authenticate.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It will redirect to your default browser and open the GitHub login page. &lt;/p&gt; &lt;p&gt;Ansible Lightspeed requires GitHub authentication. Sign in using your GitHub credentials and follow the prompts. Once done, you will be redirected to VS Code and the Connect button should now be replaced with your GitHub ID.&lt;/p&gt; &lt;h2&gt;4. Validate Ansible Lightspeed&lt;/h2&gt; &lt;p&gt;On the bottom-right corner, you should see Lightspeed (Figure 4). Ansible Lightspeed is ready for you to write playbooks.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture4.png?itok=PdGv-GJb" width="600" height="181" alt="lightspeed" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Lightspeed reflected on Visual Studio Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;5. How to trigger Ansible Lightspeed&lt;/h2&gt; &lt;p&gt;Lightspeed will recommend context based on the text you type in the name field in the playbook section. Once you hit the&lt;strong&gt; Enter &lt;/strong&gt;key, you will get the suggestion as faded text. Hit the &lt;strong&gt;Tab &lt;/strong&gt;key if everything looks good to you.&lt;/p&gt; &lt;p&gt;The first example is "Installing Minikube on an RHEL server." To generate a playbook (Figure 5), make sure you have the format of the playbook as shown below, like defining hosts, permissions of become value, and so on:&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- name: Lightspeed_demo   hosts: rhel   become: true   tasks:     - name: Install apache httpd server and enable it       ansible.builtin.package:         name: httpd         state: present     - name: Install minikube on rhel server       ansible.builtin.package:         name:           https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm         state: present&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture5.png?itok=xmindnjQ" width="600" height="405" alt="playbook-1" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Trigger Ansible Lightspeed to generate the playbook.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The second example is installing the &lt;code&gt;kubectl&lt;/code&gt; and &lt;code&gt;oc&lt;/code&gt; command-line interface (CLI) tools on a server. As in the previous example, we are defining topics about requirements in the name field, Ansible Lightspeed is generating and suggesting to us the context of Ansible playbook. If you are fine with the suggestion, press the &lt;strong&gt;Tab&lt;/strong&gt; key to accept.&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- name: Lightspeed_demo   hosts: rhel   become: true   tasks:     - name: Include redhar.rhel_system_roles.cockpit       ansible.builtin.include_role:             name: redhar.rhel_system_roles.cockpit     - name: Install apache httpd server and enable it       ansible.builtin.package:         name: httpd         state: present     - name: Install minikube on rhel server       ansible.builtin.package:         name:           https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm         state: present     - name: Install oc &amp; kubectl       ansible.builtin.package:         name: "{{ item }}"         state: present       loop:         - https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.rpm         - kubectl         - atomic-openshift-utils     - name: Install and start the openshift CRC cluster on my rhel machine       ansible.builtin.command:         cmd: /usr/local/bin/oc adm ctnplc -n openshift-cluster-api -l k8s-app=openshift-cluster-api         chdir: /home/ano-user/openshift-cluster-api&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture6.png?itok=Ij0gnTrJ" width="600" height="424" alt="playbook-2" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Red Hat OpenShift Local cluster install using Ansible Lightspeed.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Lightspeed also provides source code recommendations in the debugging window beside the terminal. It shows more information if you extend it, as shown in Figure 7.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture7.png?itok=MkA3jl7N" width="600" height="416" alt="lightspeed source" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Ansible Lightspeed code recommendation sources.  &lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your automation journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;Get started with Ansible Automation Platform by exploring interactive &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;hands-on labs&lt;/a&gt;. &lt;a href="https://developers.redhat.com/products/ansible/download#ansibleways"&gt;Download Ansible Automation Platform at no cost&lt;/a&gt; and begin your automation journey today.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/22/enhance-ansible-development-experience-lightspeed" title="Enhance Ansible development experience with Lightspeed"&gt;Enhance Ansible development experience with Lightspeed&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-08-22T07:00:00Z</dc:date></entry></feed>
