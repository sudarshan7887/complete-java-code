<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Quarkus extensions give Java dependencies superpowers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/06/introduction-quarkus-extensions-java-dependencies" /><author><name>Kevin Dubois</name></author><id>1865d737-c8ff-415b-9ed7-1b4b8fc7678a</id><updated>2023-09-06T07:00:00Z</updated><published>2023-09-06T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; extensions are one of Quarkus' best hidden-in-plain-sight features. Read on to learn how Quarkus extensions give &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; superpowers and how you can get started with them.  &lt;/p&gt; &lt;h2&gt;What are Quarkus extensions?&lt;/h2&gt; &lt;p&gt;Quarkus extensions are essentially adapter layers for Java-based libraries or technologies that enhance your application.&lt;/p&gt; &lt;p&gt;However, the scope of Quarkus extensions goes well beyond "just" importing dependent libraries. They can significantly increase the application's performance, help developers be more productive while developing their applications, integrate complex dependencies much easier, and simplify the application's source code.&lt;/p&gt; &lt;p&gt;Examples of Quarkus extensions include the Java Database Connectivity (JDBC) libraries, OpenAPI generators, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; manifest generators, and Apache Camel components. But there are many, many more.  &lt;/p&gt; &lt;h2&gt;Quarkus extensions enhance app performance&lt;/h2&gt; &lt;p&gt;A significant advantage of using Quarkus extensions is that they integrate seamlessly into the Quarkus architecture to take advantage of its superb build time optimization phase. This way, extensions can prescribe how to load and scan your application's bytecode (including the dependencies) and configuration optimally during the build augmentation stage instead of during startup time. This allows for significantly reduced resource usage and a much faster startup time for the application's dependent libraries and technologies, just like the core Quarkus components.&lt;/p&gt; &lt;p&gt;Extensions also get access to the preparation phase for GraalVM native compilation so that they can leverage the advantages of natively compiled Quarkus binaries. Keep in mind that it is the extension's responsibility to make sure it is compatible with native compilation, so make sure to read its documentation. For a deeper understanding of how this works, check out the &lt;a href="https://quarkus.io/guides/writing-extensions#technical-aspect"&gt;Quarkus documentation on how to write an extension&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Quarkus extensions increase developer productivity and joy&lt;/h2&gt; &lt;p&gt;Aside from aiding the performance of your application, Quarkus extensions can give a significant boost to developer productivity and help make developing applications more enjoyable as well by not having to fiddle around with configurations, extend the Quarkus CLI, and being able to leverage Quarkus's Dev Mode.&lt;/p&gt; &lt;h3&gt;Dev Mode&lt;/h3&gt; &lt;p&gt;Quarkus Dev Mode is a feature that starts up the application locally (or even remotely) and provides a set of capabilities to allow developers to iterate and test their code changes quickly. Dev Mode does targeted hot reloads on the fly when you change your code. This has the benefit of not needing to do manual restarts or rebuilding the application each time you change something. These hot reloads happen in a targeted manner; only the Java files that have been changed are reloaded, so there is no full init of the application.&lt;/p&gt; &lt;p&gt;This benefit applies to extensions as well. They will only be (re-)loaded if they have been changed, added, or removed. Any dependent configuration of the extension, such as a database migration script or similar, will also have the same behavior. For example, if you include an &lt;code&gt;import.sql&lt;/code&gt;, the import statement will run only during Dev Mode startup or when the file gets changed.&lt;/p&gt; &lt;h3&gt;Dev Services&lt;/h3&gt; &lt;p&gt;Extensions can also leverage the &lt;a href="https://quarkus.io/guides/dev-services"&gt;Quarkus Dev Services&lt;/a&gt; capability. Dev Services provides an automated way to spin up dependent services, typically in a local &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; and usually using the &lt;a href="https://www.testcontainers.org/"&gt;Testcontainers&lt;/a&gt; project. Think of databases, Apache Kafka clusters, Keycloak, etc. Dev Services are wired into your application without further configuration, making them a powerful way to increase developer productivity because you don't have to worry about setting up complicated dependent services on your local machine.&lt;/p&gt; &lt;p&gt;To use Dev Services, all you have to do is add the relevant extension—e.g. &lt;code&gt;jdbc-postgresql&lt;/code&gt; or &lt;code&gt;camel-quarkus-kafka&lt;/code&gt; (Figure 1)—and when you start up Quarkus's &lt;a href="https://quarkus.io/guides/dev-mode-differences#dev-mode-features"&gt;Dev Mode&lt;/a&gt; (using the command &lt;code&gt;quarkus dev&lt;/code&gt; or &lt;code&gt;mvn quarkus:dev&lt;/code&gt;), the extension will start up a Dev Service automatically. Because Dev Services usually rely on containers, you will need to have a container runtime on your machine such as Podman or Docker to enjoy this feature (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-10_15-39-19.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-10_15-39-19.png?itok=nk0JGQ38" width="600" height="84" alt="adding the camel-quarkus-kafka extensions automatically starts up a Kafka Dev Service" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Adding the camel-quarkus-kafka extension automatically starts a Kafka Dev Service during Dev Mode.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-14_08-55-32.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-14_08-55-32.png?itok=bPoYfBjO" width="600" height="310" alt="Kafka Dev Service running in Podman Desktop" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The running Kafka Dev Services container (using an image provided by Redpanda) shown in Podman Desktop&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Dev UI&lt;/h3&gt; &lt;p&gt;When Quarkus runs in Dev Mode, it also serves a web application called the Dev UI, which, among other things, shows the extensions currently used by your application. Extensions can expose additional information and capabilities in the Dev UI. At the minimum, they show details of the extension or a link to further documentation (which can be very handy!).&lt;/p&gt; &lt;p&gt;Extensions can add actionable capabilities to the Dev UI as well (Figure 3). For many extensions, you can change the configuration on the fly through the Dev UI, and some extensions have purpose-built capabilities such as building a container image, deploying to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, accessing and interacting with the Swagger UI, or even adding records to a Kafka topic running in a Dev Service. Of course, these capabilities depend on the extension and what has been implemented for it.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-11_15-58-18.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-11_15-58-18.png?itok=VluaeRxq" width="600" height="240" alt="Quarkus Dev UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Sample extensions shown in the Dev UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Extensions with a related Dev Service will also display the configuration that was automatically generated by Quarkus when the Dev Service started up. This can be very useful for developers to determine what configuration settings are needed for a particular dependency, such as the data source's type, JDBC URL, username, and password, as you can see in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-14_11-41-05.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-14_11-41-05.png?itok=NctIAfQ6" width="600" height="426" alt="Quarkus Dev UI showing Kafka and Postgresql Dev Services" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Dev Services shown in the Quarkus Dev UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Quarkus CLI&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;quarkus&lt;/code&gt; command lets you create projects, manage extensions, and do essential build and development tasks using the underlying project build tool (e.g., Maven or Gradle). Extensions can extend the functionality of the Quarkus CLI commands by virtue of Quarkus CLI plug-ins (Figure 5). This plug-in system can dynamically add commands and subcommands to the CLI. Some plug-in extensions are available out of the box, which you can retrieve with the Quarkus &lt;code&gt;plug-in list-installable&lt;/code&gt; command. In addition, you can also install executable jars from community extensions using their Maven coordinates. &lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-21_14-08-29.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-21_14-08-29.png?itok=S-kbeDfo" width="600" height="104" alt="List of Quarkus CLI plugins" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: An example list of available Quarkus CLI plug-ins.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Quarkus extensions catalog and versioning&lt;/h2&gt; &lt;p&gt;Quarkus has an extensive catalog of official Quarkus extensions. To get an idea of the extensions that are available for Quarkus, take a look at the &lt;a href="https://quarkus.io/extensions/"&gt;quarkus.io extensions catalog page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There is no need to keep track of the versioning of any individual extension, thanks to the concept of the Quarkus platform. This concept promises that any combination of the Quarkus extensions within the platform can be used in the same application without causing conflict. This also means that the versions of the extensions are determined by the platform. In practice, this works through the use of a Quarkus platform BOM artifact, which is imported as a dependency on your project.&lt;/p&gt; &lt;p&gt;Thanks to this Quarkus platform concept, you don't have to explicitly define the version of each Quarkus extension (or the libraries they are acting as a wrapper for) in your &lt;code&gt;pom.xml&lt;/code&gt; or &lt;code&gt;build.gradle&lt;/code&gt; file, and when you upgrade your Quarkus version, your extension versions are also automatically updated. Bear in mind that the extensions catalog might also contain extensions that are not necessarily production-ready, so it's always a good practice to read up on the extension's documentation before using it.&lt;/p&gt; &lt;h2&gt;The Quarkiverse&lt;/h2&gt; &lt;p&gt;The &lt;a href="http://github.com/quarkiverse"&gt;Quarkiverse GitHub organization&lt;/a&gt; provides hosting for Quarkus extension projects—not only for the ones that are featured in the main Quarkus extensions catalog, but also for additional extensions provided by Quarkus community developers. These community extensions are fully maintained by the extension's independent development teams and are typically kept up to date so that they are compatible with the different Quarkus versions. However, there are no explicit guarantees, so ensuring the project is active and up-to-date before using them is essential.&lt;/p&gt; &lt;h2&gt;How to add extensions to your Quarkus project&lt;/h2&gt; &lt;p&gt;You can add a Quarkus extension in a few ways:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;If you're generating a new project using &lt;a href="https://code.quarkus.io/"&gt;code.quarkus.io&lt;/a&gt; or the supported Red Hat build of Quarkus, &lt;a href="https://code.quarkus.redhat.com/"&gt;code.quarkus.redhat.com&lt;/a&gt;, you can select the extensions from the list and they will automatically be added to the generated code starter.&lt;/li&gt; &lt;li aria-level="1"&gt;Using the &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus CLI&lt;/a&gt;, you can add extensions using the quarkus extension add command. For example, to add the OpenShift client extension to your project, you would type &lt;code&gt;quarkus extension add quarkus-openshift-client&lt;/code&gt;. (Quarkus will automatically resolve it to &lt;code&gt;quarkus.io:quarkus-openshift-client&lt;/code&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;If you're developing with VS Code or IntelliJ IDEA, you can also use the Quarkus plug-in to add extensions to your project, as illustrated in Figure 6.&lt;/li&gt; &lt;/ul&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-10_14-40-03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-10_14-40-03.png?itok=4FhkUJVT" width="600" height="201" alt="How to add extensions with Quarkus Plugin for VS Code" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: Select "Quarkus: Add extensions to current project" in the VS Code command palette.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Go forth and extend your Quarkus application!&lt;/h2&gt; &lt;p&gt;This was a brief introduction to Quarkus extensions. If you want to learn more about Quarkus, check out the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus overview on Red Hat Developer&lt;/a&gt;, or get started with Quarkus tutorials on &lt;a href="https://developers.redhat.com/learn"&gt;developers.redhat.com/learn&lt;/a&gt;. If you want to learn even more about how Quarkus extensions work, or perhaps how to build your own extension, you can find more information on &lt;a href="https://quarkus.io/guides/writing-extensions"&gt;quarkus.io&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/06/introduction-quarkus-extensions-java-dependencies" title="Quarkus extensions give Java dependencies superpowers"&gt;Quarkus extensions give Java dependencies superpowers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Dubois</dc:creator><dc:date>2023-09-06T07:00:00Z</dc:date></entry><entry><title>A beginner's guide to Python containers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/05/beginners-guide-python-containers" /><author><name>Aine Keenan</name></author><id>ef06738c-3521-4668-8779-1dab80cd34e9</id><updated>2023-09-05T07:00:00Z</updated><published>2023-09-05T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; has emerged as a go-to language for students, new programmers, and experienced developers. This general-purpose programming language is dynamically typed, memory-managed, and supports multiple programming paradigms. Python is popular for web development, &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence and machine learning (AI/ML)&lt;/a&gt;, scripting for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;, and more.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers"&gt;Containers&lt;/a&gt; are a game-changing solution for packaging your code and dependencies, allowing for your application to run quickly and consistently across any environment. Using containers to support your Python application enables efficient development and deployment.&lt;/p&gt; &lt;p&gt;This article shows how to use containers to support your Python applications. We'll take a pre-configured application and build a Containerfile for it from scratch. Additionally, we will walk through the importance and implementation of each part.&lt;/p&gt; &lt;h2&gt;Example Flask application&lt;/h2&gt; &lt;p&gt;This tutorial uses an &lt;a href="https://github.com/ainekeenan/recipe-image.git"&gt;example Python Flask application&lt;/a&gt;. When you input a URL for a recipe, it will return links to other recipes with similar "rare" ingredients.&lt;/p&gt; &lt;p&gt;If you would like to follow along, &lt;a href="https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository"&gt;clone the example repository&lt;/a&gt; and &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;/files&lt;/code&gt; directory.&lt;/p&gt; &lt;h2&gt;Install Podman&lt;/h2&gt; &lt;p&gt;Podman is a cloud-native, daemonless tool for developing, managing, and running Linux containers. Podman manages the entire container ecosystem from pulling, building, running, and pushing an image. Podman's features are based on secure practices, but also minimize the friction between your local development environment and production.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;[ Learn more: &lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt; ] &lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Our Python container will rely on Podman to build our image, run the image, and manage the running container.&lt;/p&gt; &lt;p&gt;To install Podman:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Head to the &lt;a href="https://podman-desktop.io/"&gt;Podman Desktop&lt;/a&gt; site and select the download for your environment.&lt;/li&gt; &lt;li aria-level="1"&gt;Launch Podman Desktop.&lt;/li&gt; &lt;li aria-level="1"&gt;If not already installed, Podman Desktop will prompt you to download &lt;a href="https://docs.podman.io/en/latest/"&gt;Podman&lt;/a&gt;, the underlying container engine.&lt;/li&gt; &lt;li aria-level="1"&gt;Start the Podman machine when prompted by Podman Desktop.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now that we've configured our container engine, let’s build a Containerfile.&lt;/p&gt; &lt;h2&gt;Container parts&lt;/h2&gt; &lt;p&gt;A container will include everything your application needs to run on any machine. The main elements (illustrated in Figure 1) include a base image or platform operating system that you want the system calls to virtualize. You also need your application code. Finally, you need all dependencies, like specific versions of programming language runtimes and libraries your code needs to run.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/container_layers_.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/container_layers_.png?itok=L0W6WMOn" width="600" height="318" alt="Layers of a container: Base image, dependencies, and application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Anatomy of a container: Application code, dependencies, and a base image.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The repository for our recipe website application holds this Containerfile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;FROM python LABEL maintainer="akeenan@redhat.com" COPY dependencies.txt dependencies.txt RUN pip3 install -r dependencies.txt RUN python3 -m spacy download en_core_web_sm COPY . . CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The file &lt;code&gt;dependencies.txt&lt;/code&gt; contains the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;flask beautifulsoup4 requests numpy pandas clean-text spacy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, let's break down the Containerfile and explain why each part is necessary.&lt;/p&gt; &lt;h3&gt;FROM&lt;/h3&gt; &lt;p&gt;&lt;code&gt;FROM python&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The first necessary instruction for any Containerfile is &lt;code&gt;FROM&lt;/code&gt;. &lt;code&gt;FROM&lt;/code&gt; tells the container system to start a new build stage, or a new creation of an image, using the specified image as a base image.&lt;/p&gt; &lt;p&gt;&lt;code&gt;#Syntax for FROM &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM [--platform=] [AS ] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM [--platform=] [:] [AS ] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM [--platform=] [@] [AS ]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can use the &lt;code&gt;:&lt;tag&gt;&lt;/code&gt; to specify a specific version of an image. For example, to receive the Python version 3.9.17 image, the image argument would look like this: &lt;code&gt;python:3.9.17&lt;/code&gt;. Tags can be reused, so there is no guarantee the &lt;code&gt;python:latest&lt;/code&gt; image will be the same as &lt;code&gt;python:latest&lt;/code&gt; image in 2 months.&lt;/p&gt; &lt;p&gt;If you want to ensure that you are using the same image at all times, you can specify a digest. A digest is a specific image, specified by an ID, that will not change over time. You can obtain a digest ID in the following ways:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;By running &lt;code&gt;podman manifest inspect python:&lt;tag&gt;&lt;/code&gt; (Figure 2)&lt;/li&gt; &lt;li aria-level="1"&gt;By downloading it from various image repository hubs: &lt;ul&gt;&lt;li aria-level="2"&gt;&lt;a href="https://quay.io/repository/centos7/python-38-centos7?tab=tags"&gt;Quay&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;a href="https://hub.docker.com/_/python"&gt;Docker Hub&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;a href="https://catalog.redhat.com/software/containers/ubi8/python-38/5dde9cacbed8bd164a0af24a"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If a tag or digest is not specified, the &lt;code&gt;:latest&lt;/code&gt; tag will be used by default.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman_digest_inspect.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman_digest_inspect.png?itok=UdQ-rEjC" width="600" height="378" alt="Running Podman manifest inspect python:3.9.17 in the terminal will display the digest ID." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Running the podman manifest inspect command in the CLI to get the digest ID.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h4&gt;FROM scratch&lt;/h4&gt; &lt;p&gt;If you do not want to use a base image—perhaps you want to containerize your own underlying operating system (OS), or you have a very minimal image—you have the option to use &lt;code&gt;FROM scratch&lt;/code&gt;. The &lt;code&gt;scratch&lt;/code&gt; image does not include any files or folders; instead, it tells the container system to have the next container instruction to be the first filesystem layer of the image.&lt;/p&gt; &lt;p&gt;Each base image, except for &lt;code&gt;scratch&lt;/code&gt;, includes an OS that it virtualizes. The default &lt;code&gt;python:&lt;version&gt;&lt;/code&gt; image virtualizes Debian GNU/Linux, while &lt;code&gt;python:&lt;version&gt;-alpine&lt;/code&gt; virtualizes Alpine Linux, and &lt;code&gt;python:&lt;version&gt;-windowsservercore&lt;/code&gt; virtualizes Windows Core OS.&lt;/p&gt; &lt;p&gt;Each operating system has a &lt;strong&gt;user space&lt;/strong&gt; and a &lt;strong&gt;kernel space&lt;/strong&gt;. User spaces are processes executed by a user in the operating system. The kernel space manages resources like RAM and Disk.&lt;/p&gt; &lt;p&gt;A container runs in the user space and accesses these resources by system calls. A container abstracting a certain OS will abstract the user space, and perform system calls to the host OS. In your container, the OS virtualizes Linux distributions by having their file systems packaged into a container filesystem. Sharing an underlying virtualized OS allows for a standard environment among containers.&lt;/p&gt; &lt;p&gt;For a Python application, the Python base image works well, including all up-to-date packages you may need. &lt;code&gt;FROM python&lt;/code&gt; will instruct the container engine to inherit the Python base image. The Python base image will have an underlying OS, the language (Python source code, compiled), the Python runtime dependencies, and &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If you have a specific OS you would like to virtualize, you can inherit that base image and download Python (more on that later in this article).&lt;/p&gt; &lt;h3&gt;LABEL&lt;/h3&gt; &lt;pre&gt; &lt;code&gt;LABEL maintainer="akeenan@redhat.com" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A label is used for metadata about a container, using a key=value format. In this label, we use a maintainer key, with the value of. Using a maintainer reference provides a way for developers to reach out to the maintainer if they have questions or feedback about the container. It also gives you a way to take credit for your work.&lt;/p&gt; &lt;h3&gt;COPY&lt;/h3&gt; &lt;p&gt;&lt;code&gt;#Copy (local source) (destination in container) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;COPY dependencies.txt dependencies.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now we have our base image and a reference to a maintainer. Our container has an owner, and the basics of the Python language.&lt;/p&gt; &lt;p&gt;The next step is to use &lt;code&gt;pip&lt;/code&gt; (which is included in the Python base image) to install our dependencies. Any extra library or package a developer downloads is a dependency. In our example Python program, we imported several dependencies, including Flask, BeautifulSoup, and pandas.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;from flask import Flask, render_template, request from bs4 import BeautifulSoup import numpy as np import pandas as pd import spacy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Steps to use the &lt;code&gt;COPY&lt;/code&gt; instruction:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt; &lt;p&gt;Create a file to hold the names of all dependencies. In the sample application, it is named &lt;code&gt;dependencies.txt&lt;/code&gt; and looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;#dependencies.txt flask beautifulsoup4 requests numpy pandas clean-text spacy&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;ol start="2"&gt;&lt;li aria-level="1"&gt;The instruction &lt;code&gt;COPY&lt;/code&gt; will copy your dependency file from your local source into the container. If your file is under another directory, you can include the path or use the &lt;a href="https://www.geeksforgeeks.org/docker-workdir-instruction/"&gt;WORKDIR&lt;/a&gt; instruction.&lt;/li&gt; &lt;li aria-level="1"&gt;Inside our local directory, the dependency file is there, named &lt;code&gt;dependencies.txt&lt;/code&gt;. The instruction &lt;code&gt;COPY dependencies.txt dependencies.txt&lt;/code&gt; will create a new file named &lt;code&gt;dependencies.txt&lt;/code&gt; in your container and copy over the contents.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;RUN&lt;/h3&gt; &lt;p&gt;&lt;code&gt;RUN pip3 install -r dependencies.txt &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;RUN python3 -m spacy download en_core_web_sm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The &lt;code&gt;RUN&lt;/code&gt; instruction will execute any commands needed in a new layer on top of our image.&lt;/p&gt; &lt;p&gt;We are using Python 3, so the command in the terminal to use &lt;code&gt;pip&lt;/code&gt; to install &lt;code&gt;&lt;insert-dependency-here&gt;&lt;/code&gt; is &lt;code&gt;pip3 install -r &lt;insert-dependency-here&gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In our &lt;code&gt;dependencies.txt&lt;/code&gt; file, we have all packages and libraries that need to be downloaded using &lt;code&gt;pip&lt;/code&gt;. To shorten the installation process, &lt;code&gt;RUN pip3 install -r dependencies.txt&lt;/code&gt; will use &lt;code&gt;pip&lt;/code&gt; to install everything listed in our &lt;code&gt;dependencies.txt&lt;/code&gt; file.&lt;br /&gt;&lt;br /&gt; Additionally, for our sample program, we need to download &lt;code&gt;en_core_web_sm&lt;/code&gt; using the Python module space. This extra dependency is not installed using &lt;code&gt;pip&lt;/code&gt;, but we can use an extra &lt;code&gt;RUN&lt;/code&gt; command to complete the task.&lt;/p&gt; &lt;h3&gt;COPY&lt;/h3&gt; &lt;p&gt;&lt;code&gt;COPY . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now we have all dependencies downloaded using the &lt;code&gt;RUN&lt;/code&gt; instruction, it is time for our application files!&lt;/p&gt; &lt;p&gt;Our Containerfile is hosted in the same directory as our application files. &lt;code&gt;COPY . .&lt;/code&gt; copies our local directory outside of the container into the local directory inside the container.&lt;/p&gt; &lt;h3&gt;CMD&lt;/h3&gt; &lt;p&gt;&lt;code&gt;CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Every Containerfile must have one—and only one—&lt;code&gt;CMD&lt;/code&gt; instruction for the container to start. &lt;code&gt;CMD&lt;/code&gt; provides a default command for an executing container.&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host=0.0.0.0&lt;/code&gt; makes a non-routable meta-address of 0.0.0.0 for the host. It means that the mapping is valid for all addresses/interfaces of the host.&lt;/p&gt; &lt;p&gt;Normally, you would use the command &lt;code&gt;python3 -m flask run&lt;/code&gt; to run a Flask application. The same command is used here, in &lt;code&gt;&lt;code&gt;CMD ["executable","param1","param2"]&lt;/code&gt;&lt;/code&gt; format.&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host=0.0.0.0&lt;/code&gt; makes a non-routable meta-address of 0.0.0.0 for the host. It means that the mapping is valid for all addresses/interfaces of the host.&lt;/p&gt; &lt;h2 id="alternate_images-h2"&gt;Alternate images&lt;/h2&gt; &lt;p&gt;While the Containerfile above includes the official Python image, users can create a Containerfile to support Python in many different ways. Below, the first Containerfile inherits the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; Universal Base Image, and then installs Python. The second Containerfile inherits a base image that has combined RHEL and Python dependencies. Users can customize their Python Containerfile to work best for their dependencies and target environment. &lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;#Alternate images that work! #Red Hat Enterprise Linux UBI, then installing Python FROM redhat/ubi8 LABEL maintainer="akeenan@redhat.com" RUN yum -y install python39 COPY dependencies.txt dependencies.txt RUN pip3 install -r dependencies.txt RUN python3 -m spacy download en_core_web_sm COPY . . CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"] EXPOSE 5000 &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code&gt;#RHEL UBI + Python in base image FROM registry.access.redhat.com/ubi8/python-311:1-13 LABEL maintainer="akeenan@redhat.com" COPY dependencies.txt dependencies.txt RUN pip3 install -r dependencies.txt RUN python3 -m spacy download en_core_web_sm COPY . . CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"] EXPOSE 5000&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Build and run the container&lt;/h2&gt; &lt;p&gt;Now that you've written your image, it's time to build and run it. We'll walk through a few different ways to do this.&lt;/p&gt; &lt;h3&gt;Command-line interface&lt;/h3&gt; &lt;p&gt;In the command-line terminal, in the Containerfile directory, run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# should have your repo cloned # ensure you are in the files directory (Cd files) podman build --tag python-podman . podman run --publish 5000:5000 python-podman&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;podman build --tag python-podman .&lt;/code&gt; tells Podman to build the image in the current directory and name the image &lt;code&gt;python-podman&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;podman run --publish 5000:5000 python-podman&lt;/code&gt; will run the &lt;code&gt;python-podman&lt;/code&gt; image, and publish it on port 5000 outside of the container.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;--publish&lt;/code&gt; flag has syntax &lt;code&gt;(outside container): (inside container)&lt;/code&gt;. All Flask applications run on default port 5000, so here we connect our local host (address 127.0.0.1) port 5000 to container port 5000 on address 0.0.0.0.&lt;/p&gt; &lt;p&gt;Now, head to &lt;a href="http://127.0.0.1:5000/"&gt;http://127.0.0.1:5000&lt;/a&gt;, as seen in Figure 5, and enjoy your application! &lt;code&gt; &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Podman Desktop&lt;/h3&gt; &lt;p&gt;Add to the Containerfile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;EXPOSE 5000&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When we use the command-line interface, we force our application to be exposed on the localhost port 5000 using the command &lt;code&gt;--publish 5000:5000&lt;/code&gt;. On Podman Desktop, we just use the GUI, so including this line on our Containerfile will force our application to be exposed on the localhost port 5000.&lt;br /&gt;&lt;br /&gt; There are multiple ways to run a container file on Podman Desktop:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Build from Containerfile (Figure 3).&lt;/li&gt; &lt;li aria-level="1"&gt;Build on local terminal.&lt;/li&gt; &lt;li aria-level="1"&gt;Build from Quay.&lt;/li&gt; &lt;/ul&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_from_contianer_file.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_from_contianer_file.png?itok=uhAUCH_q" width="600" height="395" alt="Using Podman Desktop to build a image from Containerfile" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Using Podman Desktop to build a image from a Containerfile.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h4&gt;Build from Containerfile&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Navigate in Podman Desktop to the&lt;strong&gt; Images&lt;/strong&gt; tab and select &lt;strong&gt;Build an Image&lt;/strong&gt;, as shown in Figure 4. &lt;ol&gt;&lt;li aria-level="2"&gt;Select the location of your Containerfile.&lt;/li&gt; &lt;li aria-level="2"&gt;Select the location of your build directory.&lt;/li&gt; &lt;li aria-level="2"&gt;Name the image.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Build&lt;/strong&gt;, let the image build, and then select &lt;strong&gt;Done&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click the play button to start the container. &lt;ol&gt;&lt;li aria-level="2"&gt;Name the container.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the application by selecting the &lt;strong&gt;…&lt;/strong&gt; icon and &lt;strong&gt;Open in Browser&lt;/strong&gt;. You will be redirected to localhost:5000, as seen in Figure 5.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/nvm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/nvm.png?itok=Rq8CkSBs" width="600" height="395" alt="The Images tab in the Podman Desktop UI with the Pull an image and Build an image buttons." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The Images tab in the Podman Desktop where you can pull and build images.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h4&gt;Build on local terminal&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;In your build context directory, run: &lt;code&gt;podman build --tag python-podman .&lt;/code&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;In Podman Desktop, under the &lt;strong&gt;Images&lt;/strong&gt; tab, you will have a image called &lt;code&gt;localhost/python-podman.&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click the play button to start the container. &lt;ol&gt;&lt;li aria-level="2"&gt;Name the container.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the application by selecting the &lt;strong&gt;…&lt;/strong&gt; icon and &lt;strong&gt;Open in Browser&lt;/strong&gt;. You will be redirected to localhost:5000, as seen in Figure 5.&lt;/li&gt; &lt;/ol&gt;&lt;h4&gt;Build from Quay (or another image repository)&lt;/h4&gt; &lt;p&gt;If you need to set up your Quay repository, refer to the instructions in &lt;a href="https://docs.quay.io/guides/create-repo.html#:~:text=via%20the%20UI-,To%20create%20a%20repository%20in%20the%20Quay.io%20UI%2C%20click,the%20'Create%20Repository'%20button."&gt;this guide&lt;/a&gt;. &lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;In Podman Desktop, under the &lt;strong&gt;Images&lt;/strong&gt; tab, select the &lt;strong&gt;Pull an Image&lt;/strong&gt; icon, as shown in Figure 4. &lt;ol&gt;&lt;li aria-level="2"&gt;Enter the name of your image. It will look like this: &lt;code&gt;quay.io/&lt;username&gt;/&lt;image-name&gt;/&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Pull the image.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click the play button to start the container. &lt;ol&gt;&lt;li aria-level="2"&gt;Name the container.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the application by selecting the &lt;strong&gt;…&lt;/strong&gt; icon and &lt;strong&gt;Open in Browser&lt;/strong&gt;. You will be redirected to localhost:5000, as seen in Figure 5.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/recipe_app_norm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/recipe_app_norm.png?itok=DVAOA6zz" width="600" height="359" alt="The landing page for our web application on localhost:5000" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Our Flask application running on localhost:5000.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations! We successfully created a containerized Python application and saw how to run it in multiple ways. If you are working with multiple teams or multiple environments, you can now guarantee that your application will work the same every time you run it.&lt;/p&gt; &lt;p&gt;Whether you are a student, attending a hackathon, doing a personal project, or working on a business, Python and containers are a great place to start coding. Here are a few other resources to check out:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/get-started-with-your-developer-sandbox"&gt;Deploy a sample app on the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/build-and-deploy-a-quarkus-application-to-openshift-in-minutes"&gt;Build and deploy a Quarkus application to OpenShift in minutes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/using-openshift-pipelines"&gt;Using OpenShift Pipelines for automated builds and deployment&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When it comes time to scale your application, check out &lt;a href="https://developers.redhat.com/developer-sandbox/activities/learn-kubernetes-using-red-hat-developer-sandbox-openshift"&gt;Kubernetes for container orchestration&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/05/beginners-guide-python-containers" title="A beginner's guide to Python containers"&gt;A beginner's guide to Python containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Aine Keenan</dc:creator><dc:date>2023-09-05T07:00:00Z</dc:date></entry><entry><title type="html">Tutorial: Using Debezium JDBC Connector</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/debezium/tutorial-using-debezium-jdbc-connector/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/debezium/tutorial-using-debezium-jdbc-connector/</id><updated>2023-09-04T16:48:34Z</updated><content type="html">Debezium is an open-source CDC (Change Data Capture) platform that allows you to capture and stream database changes in real-time. The Debezium JDBC Connector enables you to monitor changes in relational databases, like PostgreSQL, and stream those changes to various downstream systems. In this tutorial, we will guide you through the process of using the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to use dynamic inventories in Ansible Automation</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/04/how-use-dynamic-inventories-ansible-automation" /><author><name>Deepankar Jain, Himanshu Yadav</name></author><id>1f49616c-32b2-4328-83a4-638f9319260d</id><updated>2023-09-04T07:00:00Z</updated><published>2023-09-04T07:00:00Z</published><summary type="html">&lt;p&gt;Dynamic inventories in the Red Hat Ansible Automation Platform revolutionize the way to manage infrastructure in the cloud. Instead of relying on static inventories that require manual updates when virtual machines (VMs) are launched, terminated, or replaced, dynamic inventories automatically discover and run VMs across any cloud provider. This means that when you delete an old VM and launch a new one, Ansible Automation Platform seamlessly adapts and performs actions on the updated infrastructure without requiring manual intervention.&lt;/p&gt; &lt;p&gt;By harnessing dynamic inventories, the Ansible Automation Platform empowers cloud administrators and DevOps teams to effortlessly manage and orchestrate on-demand cloud resources. This article explores the power of dynamic inventories, focusing on their utilization with AWS as the cloud provider. While applicable to any cloud provider, we showcase seamless AWS infrastructure management using dynamic inventories. We will demonstrate how to create EC2 instances, fetching details with dynamic inventory and running scripts for system health insights. By following these examples, you'll gain hands-on experience in effectively managing and monitoring your cloud infrastructure using dynamic inventories within the Ansible Automation Platform.&lt;/p&gt; &lt;h2&gt;Demo setup&lt;/h2&gt; &lt;p&gt;We will walk you through this hands-on example, creating three EC2 instances in AWS. We will then explore how to leverage dynamic inventory in the Ansible Automation Platform to automatically fetch the details of these instances. Using a playbook, we will execute tasks on these instances and observe the desired changes.&lt;/p&gt; &lt;p&gt;We will also delete one of the EC2 instances and spin up a new one, showcasing the dynamic nature of inventories. By syncing the inventory in the Ansible Automation Platform, we will observe how it seamlessly adapts to the changes, enabling us to easily continue managing the updated infrastructure.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;You must have an active AWS account.&lt;/li&gt; &lt;li aria-level="1"&gt;Generate the &lt;a href="https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html"&gt;access key and client secret for AWS&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Navigate to the &lt;strong&gt;Credentials tab&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Under the &lt;strong&gt;Add&lt;/strong&gt; &lt;strong&gt;button&lt;/strong&gt;, select &lt;strong&gt;Amazon Web Services&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Add your access key and secret key and save the credentials (Figure 1).&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-28_102411.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-28_102411.png?itok=2cfaeDZk" width="600" height="177" alt="A screenshot of the Ansible create AWS credentials page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Ansible create credentials page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;How to use dynamic inventory to manage AWS infrastructure&lt;/h2&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create three EC2 instances in the AWS console as follows:&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Log in to your AWS console and navigate to the EC2 sections.&lt;/li&gt; &lt;li aria-level="1"&gt;Under the EC2 console, click on &lt;strong&gt;Launch Instances&lt;/strong&gt; and create three instances with different names for Ubuntu image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Make sure to use a single private key for all three EC2 instances as shown in Figure 2.&lt;/p&gt; &lt;ol start="2"&gt;&lt;li aria-level="1"&gt;Create the credentials to SSH into the AWS machine as follows:&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Navigate to the credentials tab.&lt;/li&gt; &lt;li aria-level="1"&gt;Under the &lt;strong&gt;Add&lt;/strong&gt; &lt;strong&gt;button&lt;/strong&gt;, select machine.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the credential.&lt;/li&gt; &lt;li aria-level="1"&gt;Under &lt;strong&gt;Username&lt;/strong&gt;, enter “ubuntu” and enter “root” for &lt;strong&gt;Privilege Escalation Username&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Add your AWS SSH private key and click &lt;strong&gt;save&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_1.png?itok=s3tvTHNs" width="600" height="258" alt="Figure 2: Create machine credentials" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Create machine credentials.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="3"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Set up the inventory as follows:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Select the inventory from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on add and select add inventory.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the inventory and save it.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the sources from inventories and click on add.&lt;/li&gt; &lt;li aria-level="1"&gt;Give a name to the source.&lt;/li&gt; &lt;li aria-level="1"&gt;Under source, select Amazon EC2.&lt;/li&gt; &lt;li aria-level="1"&gt;In the &lt;strong&gt;Credentials field&lt;/strong&gt;, select the AWS credentials you created earlier.&lt;/li&gt; &lt;li aria-level="1"&gt;Save the sources and then click on the &lt;strong&gt;Sync&lt;/strong&gt; button.&lt;/li&gt; &lt;li aria-level="1"&gt;Now, navigate to the &lt;strong&gt;Groups&lt;/strong&gt; section and notice that &lt;strong&gt;aws_ec2&lt;/strong&gt; group is created, and your EC2 IPs will be available in the hosts (Figure 3).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_4.png?itok=yGB79Y3S" width="600" height="158" alt="Figure 4: AWS hosts in Inventory" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: AWS hosts in Inventory.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="4"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Create and Configure the project as follows:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Log in to the Ansible Automation Platform portal in the browser.&lt;/li&gt; &lt;li aria-level="1"&gt;Navigate to the &lt;strong&gt;Projects&lt;/strong&gt; tab under &lt;strong&gt;Resources&lt;/strong&gt; in the left pane.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Add&lt;/strong&gt; to create a new project.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the project and choose &lt;strong&gt;Git&lt;/strong&gt; as the source control type with URL https://github.com/redhat-developer-demos/Ansible-use-cases in the &lt;strong&gt;Source Control URL field&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Save the changes and wait for the operation to complete successfully.&lt;/li&gt; &lt;/ul&gt;&lt;ol start="5"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Create and Configure the job templates:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Go to the &lt;strong&gt;Templates&lt;/strong&gt; tab under &lt;strong&gt;Resources&lt;/strong&gt; in the left pane and click on the &lt;strong&gt;Add&lt;/strong&gt; button and select &lt;strong&gt;Job template&lt;/strong&gt; from the options.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the job you want to create and select the Demo-Inventory or Default inventory in the inventory section.&lt;/li&gt; &lt;li aria-level="1"&gt;In the &lt;strong&gt;Project&lt;/strong&gt; section, click on the project name you previously created and select the &lt;strong&gt;Dynamic Inventory in Ansible Automation Platform/get_sys_data.yml&lt;/strong&gt; file.&lt;/li&gt; &lt;li aria-level="1"&gt;In the credential section, select the machine category and choose the credentials for AWS.&lt;/li&gt; &lt;/ul&gt;&lt;ol start="6"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Launch the playbooks:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Launch the playbook to see the logs of the output as shown in Figure 4:&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_3_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_3_0.png?itok=7EuaOrQ7" width="600" height="251" alt="Figure 4: Playbook output" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The playbook output.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="7"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Delete an EC2 instance and spin up a new instance:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Now, let us delete the third machine and create a new ec2 instance.&lt;/li&gt; &lt;li aria-level="1"&gt;Under AWS console, navigate to the EC2 instance, delete any machine, and create a new instance with the previous configuration setup (Figure 5).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_5.png?itok=oDziDcxR" width="600" height="290" alt="Figure 5: Deleting an AWS machine" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Deleting an AWS machine.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="8"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Sync the inventory:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Navigate to the inventory section and click on the inventory you previously created.&lt;/li&gt; &lt;li aria-level="1"&gt;Under the &lt;strong&gt;Sources Tab&lt;/strong&gt;, select the name created earlier and click on sync.&lt;/li&gt; &lt;li aria-level="1"&gt;Further, we can also schedule the task to update the inventory after every interval.&lt;/li&gt; &lt;li aria-level="1"&gt;Notice that our hosts have been updated (Figure 6). It takes time from the AWS console to completely terminate your old EC2 instances, and it might show up under hosts.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_6.png?itok=lKq5HAJg" width="600" height="176" alt="Figure 6: The updated hosts under Ansible Automation Platform" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The updated hosts under Ansible Automation Platform.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="9"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Launch the playbook with the updated inventory:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Navigate to the &lt;strong&gt;Templates&lt;/strong&gt; section and select the job template created earlier.&lt;/li&gt; &lt;li aria-level="1"&gt;Launch the playbook (Figure 7).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_7.png?itok=mx4FpLZ3" width="600" height="260" alt="Figure 7: The playbook output with updated inventory" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: The playbook output with updated inventory.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;Dynamic inventory not only simplifies inventory management but also enhances the agility and scalability of your workflows. In this article, we have demonstrated the power of efficient infrastructure automation by showcasing the creation and management of three EC2 instances in AWS and the utilization of dynamic inventory to seamlessly adapt to changes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started&lt;/a&gt; with Ansible Automation Platform by exploring interactive hands-on labs. &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Download Ansible Automation Platform&lt;/a&gt; at no cost and begin your automation journey. You can refer to &lt;a href="https://developers.redhat.com/e-books/choosing-automation-tool"&gt;An IT executive's guide to automation&lt;/a&gt; e-book for a better understanding of automation. Additionally, check out our series where we explain &lt;a href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-cli"&gt;how to create an EC2 instance in AWS using Ansible&lt;/a&gt;, empowering you to efficiently manage your cloud resources.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/04/how-use-dynamic-inventories-ansible-automation" title="How to use dynamic inventories in Ansible Automation"&gt;How to use dynamic inventories in Ansible Automation&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain, Himanshu Yadav</dc:creator><dc:date>2023-09-04T07:00:00Z</dc:date></entry><entry><title>Automate message queue deployment on JBoss EAP</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/01/automate-message-queue-deployment-jboss-eap" /><author><name>Romain Pelisse</name></author><id>94e338a7-dd9b-4b3a-b292-0941474d3d31</id><updated>2023-09-01T07:00:00Z</updated><published>2023-09-01T07:00:00Z</published><summary type="html">&lt;p&gt;For decades now, software projects have relied on messaging &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;APIs&lt;/a&gt; to exchange data. In the &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt;/Java EE ecosystem, this method of asynchronous communication has been standardized by the JMS specification. In many cases, individuals and organizations leverage &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) to act as message-oriented middleware (MOM), which facilitates the management of message queues and topics.&lt;/p&gt; &lt;p&gt;Messaging ensures that no messages are lost as they are transmitted from the client and delivered to interested parties. On top of that, JBoss EAP provides authentication and other security-focused capabilities on top of the management functions.&lt;/p&gt; &lt;p&gt;In this article, we'll show how to fully automate the setup of JBoss EAP and a JMS queue using &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; so that we can easily make this service available.&lt;/p&gt; &lt;h2&gt;1. Prerequisites and installation&lt;/h2&gt; &lt;h3&gt;1.1 Install Ansible&lt;/h3&gt; &lt;p&gt;First, we’ll set up our Ansible &lt;strong&gt;control&lt;/strong&gt; machine, which is where the automation will be executed. On this system, we need to install Ansible as the first step:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo dnf install -y ansible-core&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the package name has changed recently from &lt;code&gt;ansible&lt;/code&gt; to &lt;code&gt;ansible-core&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;1.2 Configure Ansible to use Red Hat Automation Hub&lt;/h3&gt; &lt;p&gt;An extension to Ansible, an &lt;a href="https://docs.ansible.com/ansible/latest/collections_guide/index.html"&gt;Ansible collection&lt;/a&gt;, dedicated to Red Hat JBoss EAP is available from Automation Hub. Red Hat customers need to add credentials and the location for &lt;a href="https://www.ansible.com/products/automation-hub"&gt;Red Hat Automation Hub&lt;/a&gt; to their Ansible configuration file (&lt;code&gt;ansible.cfg&lt;/code&gt;) to be able to install the content using the &lt;a href="https://docs.ansible.com/ansible/latest/cli/ansible-galaxy.html"&gt;ansible-galaxy&lt;/a&gt; command-line tool.&lt;/p&gt; &lt;p&gt;Be sure to replace the with the API token you retrieved from Automation Hub. For more information about using &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;Red Hat Automation Hub, please refer to the associated documentation&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;#ansible.cfg: [defaults] host_key_checking = False retry_files_enabled = False nocows = 1 [inventory] # fail more helpfully when the inventory file does not parse (Ansible 2.4+) unparsed_is_failed=true [galaxy] server_list = automation_hub, galaxy [galaxy_server.galaxy] url=https://galaxy.ansible.com/ [galaxy_server.automation_hub] url=https://cloud.redhat.com/api/automation-hub/ auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token token=&lt;paste-your-token-here&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;1.3 Install the Ansible collection for JBoss EAP&lt;/h3&gt; &lt;p&gt;With this configuration, we can now install the Ansible collection for JBoss EAP (&lt;code&gt;redhat.eap&lt;/code&gt;) available on Red Hat Ansible Automation Hub:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install redhat.eap Starting galaxy collection install process Process install dependency map Starting collection install process Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/redhat-eap-1.3.4.tar.gz to /root/.ansible/tmp/ansible-local-2529rs7zh7/tmps_4n2eyj/redhat-eap-1.3.4-lr8dvcxo Installing 'redhat.eap:1.3.4' to '/root/.ansible/collections/ansible_collections/redhat/eap' Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/redhat-runtimes_common-1.1.0.tar.gz to /root/.ansible/tmp/ansible-local-2529rs7zh7/tmps_4n2eyj/redhat-runtimes_common-1.1.0-o6qfkgju redhat.eap:1.3.4 was installed successfully Installing 'redhat.runtimes_common:1.1.0' to '/root/.ansible/collections/ansible_collections/redhat/runtimes_common' Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/ansible-posix-1.5.4.tar.gz to /root/.ansible/tmp/ansible-local-2529rs7zh7/tmps_4n2eyj/ansible-posix-1.5.4-4pgukpuo redhat.runtimes_common:1.1.0 was installed successfully Installing 'ansible.posix:1.5.4' to '/root/.ansible/collections/ansible_collections/ansible/posix' ansible.posix:1.5.4 was installed successfully &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As we will describe a little later on, this extension for Ansible will manage the entire installation and configuration of the Java application server on the target systems.&lt;/p&gt; &lt;h3&gt;1.4 Inventory file&lt;/h3&gt; &lt;p&gt;Before we can start using our collection, we need to provide the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;inventory of targets to Ansible&lt;/a&gt;. There are several ways to provide this information to the automation tool, but for the purposes of this article, we elected to use a simple ini-formatted inventory file.&lt;/p&gt; &lt;p&gt;To easily reproduce this article's demonstration, you can use the same control node as the target. This also removes the need to deploy the required SSH key on all the systems involved. To do so, simply use the following inventory file by creating a file called &lt;code&gt;inventory&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;[all] localhost ansible_connection=local [messaging_servers] localhost ansible_connection=local&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;2. Deploying JBoss EAP&lt;/h2&gt; &lt;h3&gt;2.1 JBoss EAP installation&lt;/h3&gt; &lt;p&gt;Before we configure the JMS queues that will be configured by Ansible, we'll first deploy JBoss EAP. Once the server is successfully running on the target system, we'll adjust the automation to add the required configuration to set up the messaging layer. This is purely for didactic purposes.&lt;/p&gt; &lt;p&gt;Since we can leverage the content of the &lt;code&gt;redhat.eap&lt;/code&gt; collection, the playbook to install EAP and set it up as systemd service on the target system is minimal. Create a file called &lt;code&gt;eap_jms.yml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a JBoss EAP" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' collections: - redhat.eap roles: - eap_install - eap_systemd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the Ansible collection for JBoss EAP will also take care of downloading the required assets from the Red Hat Customer Portal (the archive containing the Java app server files). However, one does need to provide the credentials associated with a service account. A Red Hat customer can manage service accounts using the hybrid cloud console. Within this portal, on the &lt;a href="https://console.redhat.com/application-services/service-accounts"&gt;service accounts tab&lt;/a&gt;, you can create a new service account if one does not already exist.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The values obtained from the hybrid cloud console are sensitive and should be managed accordingly. For the purpose of this article, the value is passed to the &lt;code&gt;ansible-playbook&lt;/code&gt; command line. Alternatively, &lt;a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html"&gt;ansible-vault&lt;/a&gt; could be used to enforce additional defense mechanisms:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory -e rhn_username=&lt;client_id&gt; -e rhn_password=&lt;client_secret&gt; eap_jms.yml PLAY [Deploy a JBoss EAP] ****************************************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [redhat.eap.eap_install : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_install : Ensure prerequirements are fullfilled.] ********* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/prereqs.yml for localhost TASK [redhat.eap.eap_install : Validate credentials] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Check that required packages list has been provided.] *** ok: [localhost] TASK [redhat.eap.eap_install : Prepare packages list] ************************** skipping: [localhost] TASK [redhat.eap.eap_install : Add JDK package java-11-openjdk-headless to packages list] *** ok: [localhost] TASK [redhat.eap.eap_install : Install required packages (4)] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required local user exists.] ************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/user.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_install : Ensure workdir /opt/jboss_eap/ exists.] ********* ok: [localhost] TASK [redhat.eap.eap_install : Ensure archive_dir /opt/jboss_eap/ exists.] ***** ok: [localhost] TASK [redhat.eap.eap_install : Ensure server is installed] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Check local download archive path] ************** ok: [localhost] TASK [redhat.eap.eap_install : Set download paths] ***************************** ok: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from website: https://github.com/eap/eap/releases/download] *** skipping: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from RHN] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install/rhn.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [Download JBoss EAP from CSP] ********************************************* TASK [redhat.eap.eap_utils : Check arguments] ********************************** ok: [localhost] TASK [redhat.eap.eap_utils : Retrieve product download using JBoss Network API] *** ok: [localhost] TASK [redhat.eap.eap_utils : Determine install zipfile from search results] **** ok: [localhost] TASK [redhat.eap.eap_utils : Download Red Hat Single Sign-On] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Install server using RPM] *********************** skipping: [localhost] TASK [redhat.eap.eap_install : Check downloaded archive] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Copy archive to target nodes] ******************* changed: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Verify target archive state: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Read target directory information: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Extract files from /opt/jboss_eap//jboss-eap-7.4.0.zip into /opt/jboss_eap/.] *** changed: [localhost] TASK [redhat.eap.eap_install : Note: decompression was not executed] *********** skipping: [localhost] TASK [redhat.eap.eap_install : Read information on server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Check state of server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Set instance name] ****************************** ok: [localhost] TASK [redhat.eap.eap_install : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_install : Deploy configuration] *************************** changed: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for cumulative patch application are provided.] *** skipping: [localhost] TASK [Apply latest cumulative patch] ******************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for elytron adapter are provided.] *** skipping: [localhost] TASK [Install elytron adapter] ************************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Install server using Prospero] ****************** skipping: [localhost] TASK [redhat.eap.eap_install : Check eap install directory state] ************** ok: [localhost] TASK [redhat.eap.eap_install : Validate conditions] **************************** ok: [localhost] TASK [Ensure firewalld configuration allows server port (if enabled).] ********* skipping: [localhost] TASK [redhat.eap.eap_systemd : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check current EAP patch installed] ************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments for yaml configuration] ********* skipping: [localhost] TASK [Ensure required local user and group exists.] **************************** TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_systemd : Set destination directory for configuration] **** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance destination directory for configuration] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set base directory for instance] **************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set bind address] ******************************* ok: [localhost] TASK [redhat.eap.eap_systemd : Create basedir /opt/jboss_eap/jboss-eap-7.4//standalone for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Create deployment directories for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Include YAML configuration extension] *********** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check YAML configuration is disabled] *********** ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd envfile destination] **************** ok: [localhost] TASK [redhat.eap.eap_systemd : Determine JAVA_HOME for selected JVM RPM] ******* ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd unit file destination] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy service instance configuration: /etc//eap.conf] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy Systemd configuration for service: /usr/lib/systemd/system/eap.service] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Ensure service is started] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance eap state to started] ************** changed: [localhost] PLAY RECAP ********************************************************************* localhost : ok=59 changed=6 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the playbook has been successfully executed, we can confirm that the application server is running on the target system using the systemctl command :&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# systemctl status eap ● eap.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/eap.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2023-05-23 14:01:51 UTC; 1min 25s ago Main PID: 1563 (standalone.sh) Tasks: 84 (limit: 1638) Memory: 376.9M CGroup: /system.slice/eap.service ├─1563 /bin/sh /opt/jboss_eap/jboss-eap-7.4/bin/standalone.sh -c eap.xml -b 0.0.0.0 -bmanagement 127.0.0.1 -Djboss.bind.address.private=127.0.0.1 -Djboss.default.multicast.address=230.0.0.4 -Djboss.server.config.dir=/opt/jboss_eap/jboss-eap-7.4//standalone/configuration/ -Djboss.server.base.dir=/opt/jboss_eap/jboss-eap-7.4//standalone -Djboss.tx.node.id=eap -Djboss.&gt; └─1706 /usr/lib/jvm/java-11-openjdk-11.0.19.0.7-4.el8.x86_64/bin/java -D[Standalone] -server -Xlog:gc*:file=/opt/jboss_eap/jboss-eap-7.4/standalone/log/gc.log:time,uptimemillis:filecount=5,filesize=3M -Xmx1024M -Xms512M --add-exports=java.base/sun.nio.ch=ALL-UNNAMED --add-exports=jdk.unsupported/sun.misc=ALL-UNNAMED --add-exports=jdk.unsupported/sun.reflect=ALL-UNNA&gt; May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,778 INFO [org.jboss.as.connector.subsystems.datasources] (MSC service thread 1-3) WFLYJCA0001: Bound data source [java:jboss/datasources/ExampleDS] May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,852 INFO [org.jboss.as.patching] (MSC service thread 1-3) WFLYPAT0050: JBoss EAP cumulative patch ID is: base, one-off patches include: none May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,862 WARN [org.jboss.as.domain.management.security] (MSC service thread 1-4) WFLYDM0111: Keystore /opt/jboss_eap/jboss-eap-7.4/standalone/configuration/application.keystore not found, it will be auto generated on first use with a self signed certificate for host localhost May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,867 INFO [org.jboss.as.server.deployment.scanner] (MSC service thread 1-5) WFLYDS0013: Started FileSystemDeploymentService for directory /opt/jboss_eap/jboss-eap-7.4/standalone/deployments May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,882 INFO [org.wildfly.extension.undertow] (MSC service thread 1-7) WFLYUT0006: Undertow HTTPS listener https listening on [0:0:0:0:0:0:0:0]:8443 May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,920 INFO [org.jboss.ws.common.management] (MSC service thread 1-4) JBWS022052: Starting JBossWS 5.4.2.Final-redhat-00001 (Apache CXF 3.3.9.redhat-00001) May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,989 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming server May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,991 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: JBoss EAP 7.4.0.GA (WildFly Core 15.0.2.Final-redhat-00001) started in 2282ms - Started 317 of 556 services (343 services are lazy, passive or on-demand) May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,992 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/management May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,992 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2.2 Validating the installation&lt;/h3&gt; &lt;p&gt;Before going any further with our automation, we will be thorough and add a validation step to double-check that the application server is not only running but also functional. This will ensure, down the road, that any JMS-related issue only affects this subsystem.&lt;/p&gt; &lt;p&gt;The Ansible collection for JBoss EAP comes with a handy role, called &lt;code&gt;eap_validation&lt;/code&gt;, for this purpose, so it's fairly easy to add this step to our playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a JBoss EAP" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' collections: - redhat.eap roles: - eap_install - eap_systemd - eap_validation&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's execute our playbook once again and observe the execution of this validation step:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory -e rhn_username=&lt;client_id&gt; -e rhn_password=&lt;client_secret&gt; eap_jms.yml PLAY [Deploy a JBoss EAP] ****************************************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [redhat.eap.eap_install : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_install : Ensure prerequirements are fullfilled.] ********* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/prereqs.yml for localhost TASK [redhat.eap.eap_install : Validate credentials] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Check that required packages list has been provided.] *** ok: [localhost] TASK [redhat.eap.eap_install : Prepare packages list] ************************** skipping: [localhost] TASK [redhat.eap.eap_install : Add JDK package java-11-openjdk-headless to packages list] *** ok: [localhost] TASK [redhat.eap.eap_install : Install required packages (4)] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required local user exists.] ************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/user.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** changed: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ changed: [localhost] TASK [redhat.eap.eap_install : Ensure workdir /opt/jboss_eap/ exists.] ********* changed: [localhost] TASK [redhat.eap.eap_install : Ensure archive_dir /opt/jboss_eap/ exists.] ***** ok: [localhost] TASK [redhat.eap.eap_install : Ensure server is installed] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Check local download archive path] ************** ok: [localhost] TASK [redhat.eap.eap_install : Set download paths] ***************************** ok: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from website: https://github.com/eap/eap/releases/download] *** skipping: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from RHN] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install/rhn.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [Download JBoss EAP from CSP] ********************************************* TASK [redhat.eap.eap_utils : Check arguments] ********************************** ok: [localhost] TASK [redhat.eap.eap_utils : Retrieve product download using JBoss Network API] *** ok: [localhost] TASK [redhat.eap.eap_utils : Determine install zipfile from search results] **** ok: [localhost] TASK [redhat.eap.eap_utils : Download Red Hat Single Sign-On] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Install server using RPM] *********************** skipping: [localhost] TASK [redhat.eap.eap_install : Check downloaded archive] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Copy archive to target nodes] ******************* changed: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Verify target archive state: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Read target directory information: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Extract files from /opt/jboss_eap//jboss-eap-7.4.0.zip into /opt/jboss_eap/.] *** changed: [localhost] TASK [redhat.eap.eap_install : Note: decompression was not executed] *********** skipping: [localhost] TASK [redhat.eap.eap_install : Read information on server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Check state of server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Set instance name] ****************************** ok: [localhost] TASK [redhat.eap.eap_install : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_install : Deploy configuration] *************************** changed: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for cumulative patch application are provided.] *** skipping: [localhost] TASK [Apply latest cumulative patch] ******************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for elytron adapter are provided.] *** skipping: [localhost] TASK [Install elytron adapter] ************************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Install server using Prospero] ****************** skipping: [localhost] TASK [redhat.eap.eap_install : Check eap install directory state] ************** ok: [localhost] TASK [redhat.eap.eap_install : Validate conditions] **************************** ok: [localhost] TASK [Ensure firewalld configuration allows server port (if enabled).] ********* skipping: [localhost] TASK [redhat.eap.eap_systemd : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check current EAP patch installed] ************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments for yaml configuration] ********* skipping: [localhost] TASK [Ensure required local user and group exists.] **************************** TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_systemd : Set destination directory for configuration] **** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance destination directory for configuration] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set base directory for instance] **************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set bind address] ******************************* ok: [localhost] TASK [redhat.eap.eap_systemd : Create basedir /opt/jboss_eap/jboss-eap-7.4//standalone for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Create deployment directories for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Include YAML configuration extension] *********** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check YAML configuration is disabled] *********** ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd envfile destination] **************** ok: [localhost] TASK [redhat.eap.eap_systemd : Determine JAVA_HOME for selected JVM RPM] ******* ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd unit file destination] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy service instance configuration: /etc//eap.conf] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy Systemd configuration for service: /usr/lib/systemd/system/eap.service] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Ensure service is started] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance eap state to started] ************** changed: [localhost] TASK [redhat.eap.eap_validation : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of user: eap] ***************** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of group: eap.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Wait for HTTP port 8080 to become available.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Check if web connector is accessible] ******** ok: [localhost] TASK [redhat.eap.eap_validation : Populate service facts] ********************** ok: [localhost] TASK [redhat.eap.eap_validation : Check if service is running] ***************** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [redhat.eap.eap_validation : Verify server's internal configuration] ****** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/core-service=server-environment:read-attribute(name=start-gracefully)'}) included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'}) TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /core-service=server-environment:read-attribute(name=start-gracefully)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/core-service=server-environment:read-attribute(name=start-gracefully)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure yaml setup] *************************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/yaml_setup.yml for localhost TASK [Check standard-sockets configuration settings] *************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=mail-smtp:read-attribute(name=host)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of standard-sockets configuration settings] *** ok: [localhost] TASK [Check ejb configuration settings] **************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ejb3:read-attribute(name=default-resource-adapter-name)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ejb configuration settings] *** ok: [localhost] TASK [Check ee configuration settings] ***************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ee/service=default-bindings:read-attribute(name=jms-connection-factory)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ee configuration settings] *** ok: [localhost] PLAY RECAP ********************************************************************* localhost : ok=98 changed=9 unreachable=0 failed=0 skipped=24 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the execution of the playbook completed without error, validation of the application server passed successfully.&lt;/p&gt; &lt;h2&gt;3 Deploying JMS queues on JBoss EAP using Ansible&lt;/h2&gt; &lt;h3&gt;3.1 Changing EAP configuration&lt;/h3&gt; &lt;p&gt;Because the JMS subsystem is not used in the default JBoss EAP server configuration (&lt;code&gt;standalone.xml&lt;/code&gt;), we also need to use a different profile (&lt;code&gt;standalone-alone.xml&lt;/code&gt;). This is why, in the playbook above, we are specifying the required configuration profile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a JBoss EAP" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' collections: - redhat.eap roles: - eap_install - eap_systemd - eap_validation&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3.2 Leveraging the YAML config feature of EAP using Ansible&lt;/h3&gt; &lt;p&gt;In the previous section, JBoss EAP was installed and configured as a systemd service on the target systems. Now, we will update this automation to change the configuration of the app server to ensure a JMS queue is deployed and made available.&lt;/p&gt; &lt;p&gt;In order to accomplish this goal, we just need to provide a YAML definition with the appropriate configuration for the JMS subsystem of JBoss EAP. This configuration file is used by the app server, on boot, to update its configuration.&lt;/p&gt; &lt;p&gt;To achieve this, we need to add another file to our project that we named &lt;code&gt;jms_configuration.yml.j2&lt;/code&gt;. While the content of the file itself is YAML, the extension is &lt;code&gt;.j2&lt;/code&gt; because it's a jinja2 template, which allows us to take advantage of the advanced, dynamic capabilities provided by Ansible.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;jms_configuration.yml.j2: wildfly-configuration: subsystem: messaging-activemq: server: default: jms-queue: {{ queue.name }}: entries: - '{{ queue.entry }}' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Below, you'll see the playbook updated with all the required parameters to deploy the JMQ queue on JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a Red Hat JBoss EAP server and set up a JMS Queue" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' eap_enable_yml_config: True queue: name: MyQueue entry: 'java:/jms/queue/MyQueue' eap_yml_configs: - jms_configuration.yml.j2 collections: - redhat.eap roles: - eap_install - eap_systemd - eap_validation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's execute this playbook again:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory -e rhn_username=&lt;client_id&gt; -e rhn_password=&lt;client_secret&gt; eap_jms.yml PLAY [Deploy a Red Hat JBoss EAP server and set up a JMS Queue] **************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [redhat.eap.eap_install : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_install : Ensure prerequirements are fullfilled.] ********* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/prereqs.yml for localhost TASK [redhat.eap.eap_install : Validate credentials] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Check that required packages list has been provided.] *** ok: [localhost] TASK [redhat.eap.eap_install : Prepare packages list] ************************** skipping: [localhost] TASK [redhat.eap.eap_install : Add JDK package java-11-openjdk-headless to packages list] *** ok: [localhost] TASK [redhat.eap.eap_install : Install required packages (4)] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required local user exists.] ************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/user.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_install : Ensure workdir /opt/jboss_eap/ exists.] ********* ok: [localhost] TASK [redhat.eap.eap_install : Ensure archive_dir /opt/jboss_eap/ exists.] ***** ok: [localhost] TASK [redhat.eap.eap_install : Ensure server is installed] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Check local download archive path] ************** ok: [localhost] TASK [redhat.eap.eap_install : Set download paths] ***************************** ok: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from website: https://github.com/eap/eap/releases/download] *** skipping: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from RHN] ********************** skipping: [localhost] TASK [redhat.eap.eap_install : Install server using RPM] *********************** skipping: [localhost] TASK [redhat.eap.eap_install : Check downloaded archive] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Copy archive to target nodes] ******************* skipping: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Verify target archive state: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Read target directory information: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Extract files from /opt/jboss_eap//jboss-eap-7.4.0.zip into /opt/jboss_eap/.] *** skipping: [localhost] TASK [redhat.eap.eap_install : Note: decompression was not executed] *********** ok: [localhost] =&gt; { "msg": "/opt/jboss_eap/jboss-eap-7.4/ already exists and version unchanged, skipping decompression" } TASK [redhat.eap.eap_install : Read information on server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Check state of server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Set instance name] ****************************** ok: [localhost] TASK [redhat.eap.eap_install : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_install : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for cumulative patch application are provided.] *** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [Apply latest cumulative patch] ******************************************* TASK [redhat.eap.eap_utils : Check installation] ******************************* ok: [localhost] TASK [redhat.eap.eap_utils : Set patch directory] ****************************** ok: [localhost] TASK [redhat.eap.eap_utils : Set download patch archive path] ****************** ok: [localhost] TASK [redhat.eap.eap_utils : Set patch destination directory] ****************** ok: [localhost] TASK [redhat.eap.eap_utils : Check download patch archive path] **************** ok: [localhost] TASK [redhat.eap.eap_utils : Check local download archive path] **************** ok: [localhost] TASK [redhat.eap.eap_utils : Check local downloaded archive: jboss-eap-7.4.9-patch.zip] *** ok: [localhost] TASK [redhat.eap.eap_utils : Retrieve product download using JBossNetwork API] *** skipping: [localhost] TASK [redhat.eap.eap_utils : Determine patch versions list] ******************** skipping: [localhost] TASK [redhat.eap.eap_utils : Determine latest version] ************************* skipping: [localhost] TASK [redhat.eap.eap_utils : Determine install zipfile from search results] **** skipping: [localhost] TASK [redhat.eap.eap_utils : Determine selected patch from supplied version: 7.4.9] *** skipping: [localhost] TASK [redhat.eap.eap_utils : Check remote downloaded archive: /opt/jboss-eap-7.4.9-patch.zip] *** skipping: [localhost] TASK [redhat.eap.eap_utils : Download Red Hat EAP patch] *********************** skipping: [localhost] TASK [redhat.eap.eap_utils : Set download patch archive path] ****************** ok: [localhost] TASK [redhat.eap.eap_utils : Check remote download patch archive path] ********* ok: [localhost] TASK [redhat.eap.eap_utils : Copy patch archive to target nodes] *************** changed: [localhost] TASK [redhat.eap.eap_utils : Check patch state] ******************************** ok: [localhost] TASK [redhat.eap.eap_utils : Set checksum file path for patch] ***************** ok: [localhost] TASK [redhat.eap.eap_utils : Check /opt/jboss_eap/jboss-eap-7.4//.applied_patch_checksum_f641b6de2807fac18d2a56de7a27c1ea3611e5f3.txt state] *** ok: [localhost] TASK [redhat.eap.eap_utils : Print when patch has been applied already] ******** skipping: [localhost] TASK [redhat.eap.eap_utils : Check if management interface is reachable] ******* ok: [localhost] TASK [redhat.eap.eap_utils : Set apply CP conflict default strategy to default (if not defined): --override-all] *** ok: [localhost] TASK [redhat.eap.eap_utils : Apply patch /opt/jboss-eap-7.4.9-patch.zip to server installed in /opt/jboss_eap/jboss-eap-7.4/] *** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_utils/tasks/jboss_cli.yml for localhost TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query 'patch apply --override-all /opt/jboss-eap-7.4.9-patch.zip'] *** ok: [localhost] TASK [redhat.eap.eap_utils : Display patching result] ************************** ok: [localhost] =&gt; { "msg": "Apply patch operation result: {\n \"outcome\" : \"success\",\n \"response-headers\" : {\n \"operation-requires-restart\" : true,\n \"process-state\" : \"restart-required\"\n }\n}" } TASK [redhat.eap.eap_utils : Set checksum file] ******************************** changed: [localhost] TASK [redhat.eap.eap_utils : Set latest patch file] **************************** changed: [localhost] TASK [redhat.eap.eap_utils : Restart server to ensure patch content is running] *** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_utils/tasks/jboss_cli.yml for localhost TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query 'shutdown --restart'] *********** ok: [localhost] TASK [redhat.eap.eap_utils : Wait for management interface is reachable] ******* ok: [localhost] TASK [redhat.eap.eap_utils : Stop service if it was started for patching] ****** skipping: [localhost] TASK [redhat.eap.eap_utils : Display resulting output] ************************* skipping: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for elytron adapter are provided.] *** skipping: [localhost] TASK [Install elytron adapter] ************************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Install server using Prospero] ****************** skipping: [localhost] TASK [redhat.eap.eap_install : Check eap install directory state] ************** ok: [localhost] TASK [redhat.eap.eap_install : Validate conditions] **************************** ok: [localhost] TASK [Ensure firewalld configuration allows server port (if enabled).] ********* skipping: [localhost] TASK [redhat.eap.eap_systemd : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check current EAP patch installed] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments for yaml configuration] ********* ok: [localhost] TASK [Ensure required local user and group exists.] **************************** TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_systemd : Set destination directory for configuration] **** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance destination directory for configuration] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set base directory for instance] **************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set bind address] ******************************* ok: [localhost] TASK [redhat.eap.eap_systemd : Create basedir /opt/jboss_eap/jboss-eap-7.4//standalone for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Create deployment directories for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Include YAML configuration extension] *********** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/yml_config.yml for localhost TASK [redhat.eap.eap_systemd : Create YAML configuration directory] ************ skipping: [localhost] TASK [redhat.eap.eap_systemd : Enable YAML configuration extension] ************ skipping: [localhost] TASK [redhat.eap.eap_systemd : Create YAML configuration directory] ************ changed: [localhost] TASK [redhat.eap.eap_systemd : Enable YAML configuration extension] ************ changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy YAML configuration files] **************** changed: [localhost] =&gt; (item=jms_configuration.yml.j2) TASK [redhat.eap.eap_systemd : Check YAML configuration is disabled] *********** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set systemd envfile destination] **************** ok: [localhost] TASK [redhat.eap.eap_systemd : Determine JAVA_HOME for selected JVM RPM] ******* ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd unit file destination] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy service instance configuration: /etc//eap.conf] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy Systemd configuration for service: /usr/lib/systemd/system/eap.service] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Ensure service is started] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance eap state to started] ************** ok: [localhost] TASK [redhat.eap.eap_validation : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of user: eap] ***************** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of group: eap.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Wait for HTTP port 8080 to become available.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Check if web connector is accessible] ******** ok: [localhost] TASK [redhat.eap.eap_validation : Populate service facts] ********************** ok: [localhost] TASK [redhat.eap.eap_validation : Check if service is running] ***************** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [redhat.eap.eap_validation : Verify server's internal configuration] ****** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/core-service=server-environment:read-attribute(name=start-gracefully)'}) included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'}) TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /core-service=server-environment:read-attribute(name=start-gracefully)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/core-service=server-environment:read-attribute(name=start-gracefully)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure yaml setup] *************************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/yaml_setup.yml for localhost TASK [Check standard-sockets configuration settings] *************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=mail-smtp:read-attribute(name=host)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of standard-sockets configuration settings] *** ok: [localhost] TASK [Check ejb configuration settings] **************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ejb3:read-attribute(name=default-resource-adapter-name)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ejb configuration settings] *** ok: [localhost] TASK [Check ee configuration settings] ***************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ee/service=default-bindings:read-attribute(name=jms-connection-factory)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ee configuration settings] *** ok: [localhost] RUNNING HANDLER [redhat.eap.eap_systemd : Restart Wildfly] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost RUNNING HANDLER [redhat.eap.eap_systemd : Check arguments] ********************* ok: [localhost] RUNNING HANDLER [redhat.eap.eap_systemd : Set instance eap state to restarted] *** changed: [localhost] PLAY RECAP ********************************************************************* localhost : ok=127 changed=8 unreachable=0 failed=0 skipped=34 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As illustrated in the output above, the YAML definition is now enabled and the configuration of the JBoss EAP running on the target host has been updated.&lt;/p&gt; &lt;h3&gt;3.3 Validate the JMS queue deployment&lt;/h3&gt; &lt;p&gt;As always, we are going to be thorough and verify that the playbook execution has, indeed, properly set up a JMS queue. To do so, we can simply use the JBoss CLI provided with JBoss EAP to confirm:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ /opt/jboss_eap/jboss-eap-7.4/bin/jboss-cli.sh --connect --command="/subsystem=messaging-activemq/server=default/jms-queue=MyQueue:read-resource" { "outcome" =&gt; "success", "result" =&gt; { "durable" =&gt; true, "entries" =&gt; ["queues/MyQueue"], "legacy-entries" =&gt; undefined, "selector" =&gt; undefined } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output, as shown above, confirms that the server configuration has indeed been updated and that a brand new JMS queue is now available. Since this verification is fairly easy to automate, we will also add it to our playbook.&lt;/p&gt; &lt;p&gt;The Ansible collection for JBoss EAP comes with a handy wrapper allowing for the execution of the JBoss CLI within a playbook. So, all that is needed is the inclusion of the task and the desired command, as shown below:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;post_tasks: - name: "Check that Queue {{ queue.name }} is available." ansible.builtin.include_role: name: eap_utils tasks_from: jboss_cli.yml vars: jboss_home: "{{ eap_home }}" jboss_cli_query: "/subsystem=messaging-activemq/server=default/jms-queue={{ queue.name }}:read-resource"&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Thanks to the Ansible collection for JBoss EAP, we have a minimalistic playbook, spared of all the heavy lifting of managing the Java application server, fulfilling the role of MOM. All the configuration required by the automation concerns only the use case we tried to implement, not the inner working of the solution (JBoss EAP). All the configuration required by the automation concerns only the use case we tried to implement, not the inner working of the solution (JBoss EAP). The resulting playbook is safely repeatable and can be used to install the software on any number of target systems. Using the collection for JBoss EAP also makes it easy to keep the deployment up to date.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/01/automate-message-queue-deployment-jboss-eap" title="Automate message queue deployment on JBoss EAP"&gt;Automate message queue deployment on JBoss EAP&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2023-09-01T07:00:00Z</dc:date></entry><entry><title>How we ensure statically linked applications stay that way</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/31/how-we-ensure-statically-linked-applications-stay-way" /><author><name>Arjun Shankar</name></author><id>f47b49a2-c3b2-480f-ad54-b943d9caebdf</id><updated>2023-08-31T07:00:00Z</updated><published>2023-08-31T07:00:00Z</published><summary type="html">&lt;p&gt;While glibc's highly configurable name resolution and character set handling features offer an advantage when it comes to system configuration and installed content, there are limitations when it comes to statically linked applications. This article summarizes the current state, recent improvements, and plans for moving toward truly statically linked applications.&lt;/p&gt; &lt;p&gt;Although dynamic linking has advantages, making it the default choice for situations where binary compatibility is guaranteed (i.e., many &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux&lt;/a&gt; components), static linking is still useful in many situations such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Application developers wanting to ensure that their binaries run on a wide range of operating system vendors and versions.&lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Container introspection tools injected via oc-inject might bring their own dependencies into the system via static linking to avoid depending on in-container libraries.&lt;/li&gt; &lt;li aria-level="1"&gt;Languages such as Go might produce statically linked binaries by default.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Statically linking against glibc&lt;/h2&gt; &lt;p&gt;Regardless of the use case, when you statically link an application, the reasonable expectation is that all dependencies are linked statically. When it comes to glibc, this is currently not the case due to several glibc features, such as:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Name service switch (nsswitch)&lt;/li&gt; &lt;li aria-level="1"&gt;Character set handling and conversion (iconv)&lt;/li&gt; &lt;li aria-level="1"&gt;Thread cancellation (unwinder)&lt;/li&gt; &lt;li aria-level="1"&gt;Internationalized domain names (libidn2)&lt;/li&gt; &lt;li aria-level="1"&gt;Dynamic library loading from static code (dlopen)&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The first two features allow glibc to be highly configurable when it comes to name resolution and character set handling. There are several non-glibc name resolution services implemented in the form of NSS modules. For example, systemd provides a caching DNS resolver installed by default on Fedora. Along the same lines, while vendor plugins for character set handling are less common, glibc's fairly exhaustive list of character set converters can be shipped and installed separately, reducing system footprint.&lt;/p&gt; &lt;p&gt;These features also come with a limitation related to statically linked applications. The application can potentially dynamically load (dlopen) NSS or iconv modules during execution, depending on system configuration and module availability. A documented fact which still goes against the principle of least surprise. In the case of Go, while it tries to generate statically linked binaries by default, linking against glibc for NSS ends up making &lt;a data-sk="tooltip_parent" data-stringify-link="https://mt165.co.uk/blog/static-link-go/" href="https://mt165.co.uk/blog/static-link-go/" target="_blank"&gt;"many Go programmes dynamically-linked, to the point that a lot of people think Go dynamically links by default / preference&lt;/a&gt;."&lt;/p&gt; &lt;h2&gt;Recent changes&lt;/h2&gt; &lt;p&gt;The GNU C library offers an --enable-static-nss configure time option that builds libc.a (i.e., statically linked glibc) in a way that NSS modules are statically linked. However, a relatively recent refactor of NSS code removed this feature. It was never enabled for Fedora’s or RHEL’s glibc-static package. Since the refactor, the DNS and files back-ends (the most frequently used) have been moved into glibc instead of being shipped as separate modules. This means that doing name lookups from /etc/hosts or /etc/passwd does not lead to an NSS module load as long as /etc/nsswitch.conf only lists the files and DNS database providers. This has left the --enable-static-nss configure option redundant. However, there is more to be done here.&lt;/p&gt; &lt;h2&gt;Iterating toward a solution&lt;/h2&gt; &lt;p&gt;While there are five subsystems in this problem area, I have picked two to begin the process of iterating toward a solution: NSS and iconv.&lt;/p&gt; &lt;p&gt;The next step toward avoiding surprising NSS module loads in statically linked applications is to either bring back the functionality that --enable-static-nss provides or do even better by allowing configuring behavior at application build-time instead of distribution build-time and providing a way to completely suppress all dynamic loading, regardless of the contents of nsswitch.conf requiring it.&lt;/p&gt; &lt;p&gt;I'm currently in the process of trying to solve this. I did some initial experimentation by trying to build a new statically linked library that contains copies of glibc internal functions involved in NSS module loading, but with the module loading disabled. The idea being that with the appropriate linker command line, this can be used to link the alternative (non module loading) versions of the NSS component instead of the default versions that call dlopen. Once I had an idea of the changes we are looking at, I &lt;a href="https://sourceware.org/pipermail/libc-alpha/2023-May/148682.html"&gt;started a conversation upstream&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I am now working through the implementation details, starting with collecting NSS code scattered across the glibc source tree into one place. Once I am finished with that, I will start posting patches upstream and working toward consensus.&lt;/p&gt; &lt;p&gt;Please stay tuned for my future article describing glibc's progress on supporting truly statically linked applications.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/31/how-we-ensure-statically-linked-applications-stay-way" title="How we ensure statically linked applications stay that way"&gt;How we ensure statically linked applications stay that way&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Arjun Shankar</dc:creator><dc:date>2023-08-31T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.43.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/08/kogito-1-43-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/08/kogito-1-43-0-released.html</id><updated>2023-08-30T07:33:24Z</updated><content type="html">We are glad to announce that the Kogito 1.43.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Track user identity in Kogito events * Allow PostgreSQL persistence for process instance state when using SW executor * New endpoints REST to retrieve workflows and schema For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.31.0 artifacts are available at the . A detailed changelog for 1.43.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Go for C++ developers: A beginner's guide</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/30/go-c-developers-beginners-guide" /><author><name>Stan Cox</name></author><id>a5d2e708-3c23-4b3c-8652-ee60cb83da21</id><updated>2023-08-30T07:00:00Z</updated><published>2023-08-30T07:00:00Z</published><summary type="html">&lt;p&gt;After years of working on software written in &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt;, I switched to working on a project that is implemented in &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;. More developers may find themselves working in the Go ecosystem as more software, such as &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, is implemented in Go. This article discusses the primary language differences between Go and C++, differences in the development environments, and differences in the program-building environment. Examples and code snippets are from the Grafana sources.&lt;/p&gt; &lt;h2&gt;Program syntax&lt;/h2&gt; &lt;p&gt;Go statements are terminated by a semicolon. Go treats the end of a non-blank line as a semicolon unless it can be determined that the line is incomplete. As a result, go requires the opening brace to be on the same line as the function definition:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; func (s *Server) init() error { ... } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go requires the closing &lt;code&gt;then&lt;/code&gt; brace to be on the same line as the &lt;code&gt;else&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;if hs.Cfg.UnifiedAlerting.IsEnabled() { notifiersAuthHandler = reqSignedIn } else { notifiersAuthHandler = reqEditorRole }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Variable and type declaration&lt;/h2&gt; &lt;p&gt;Go declarations start with the keyword &lt;code&gt;var&lt;/code&gt; followed by the name and then the type, which is the opposite of C. A series of declarations may be placed in parentheses.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; var dashboardId int64 var ( hasPublicDashboard bool err error ) // Go uses the clearer form: var r1, r2 *regexp.Regexp // whereas the C equivalent would be: regexp.Regexp *r1, *r2;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A variable may be initialized when it is declared. If the type is not specified, the variable type will be the type of the initialization expression. var msg = "unknown" which will define msg as a string.&lt;/p&gt; &lt;p&gt;A short declaration syntax is allowed within a function or loop. One or more variables may be defined this way. If multiple variables are defined using this method, then one of the variables may have already been declared. This is often used to define and use an error variable.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;req, err := http.NewRequest("GET", url, nil) ... resp, err := client.Do(req)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A type declaration defines a new named type. A common use is to create a type for a defined structure.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;type Leaf struct { ... } type patternType int8 leaves []*Leaf typ patternType&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Variable assignment&lt;/h2&gt; &lt;p&gt;Go has a blank identifier, which causes the return value of the right side of the assignment to be ignored.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;_ = l.next()&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go permits multiple assignments, which are done in parallel.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;i, j = j, i // Swap i and j.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any declared but not explicitly initialized variable is automatically initialized to the zero value of the type: false for booleans, 0 for numeric types,"" for strings, and nil for other types. A variable must either be initialized or the type must be specified.&lt;/p&gt; &lt;p&gt;Go, unlike C++, requires that two variables can only be compared or assigned if their type definitions match. Go does not support implicit type conversion.&lt;/p&gt; &lt;p&gt;Go constants can be typed or untyped. If the type is present, then the expressions must be assignable to that type. If the type is omitted, the constant takes the type of the expression. An untyped numeric constant represent values using arbitrary precision.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;const eof = -1 // untyped integer const orgID int64 = 2 // int64 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Data containers&lt;/h2&gt; &lt;p&gt;Go does not support enums. Instead, you can use the special name iota in a constant declaration that represents successive untyped integer constants.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;type State int const ( Normal State = iota Alerting Pending NoData Error )&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Arrays in Go are first-class values. When an array is used as a function parameter, the function receives a copy of the array, not a pointer to it. However, in practice, functions often use slices for parameters; slices hold pointers to underlying arrays.&lt;/p&gt; &lt;p&gt;A slice can be understood to be a struct with three fields: an array pointer, a length, and a maximum size. Slices use the &lt;code&gt;[]&lt;/code&gt; operator to access elements of the underlying array. The &lt;code&gt;len&lt;/code&gt; function returns the length of the slice, and the cap function returns the maximum size.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;name = name[lastslash+1:] encrypted := []byte{0x2a, 0x59, 0x57, 0x56, 0x78}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Given an array or another slice, a new slice is created via:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;newarr := arr[i:j]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;where &lt;code&gt;newarr&lt;/code&gt; starts at index i and ends prior to index j. &lt;code&gt;newarr&lt;/code&gt; refers to &lt;code&gt;arr&lt;/code&gt;, thus changes made to &lt;code&gt;newarr&lt;/code&gt; are reflected in &lt;code&gt;arr&lt;/code&gt;. An array pointer can be assigned to a variable of slice type:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;var s []int ; var a[10] int ; s = &amp;a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A slice is similar to &lt;code&gt;std::vector&lt;/code&gt; in C++.&lt;/p&gt; &lt;p&gt;Hash tables are provided by the language. They are called maps. A map is an unordered collection of key-value pairs where the keys are unique. It is written as &lt;code&gt;map[key_type]value_type,&lt;/code&gt;where key_type is the type of the map key and value_type is the type of the map value.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;encrypted := make(map[string][]byte) encrypted[key] = encryptedData&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A Go map is similar to &lt;code&gt;std::unordered_map&lt;/code&gt; in C++.&lt;/p&gt; &lt;p&gt;Strings are provided by the language. They are immutable and cannot be changed once they have been created.&lt;/p&gt; &lt;h2&gt;Expressions&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;for&lt;/code&gt; statement is the only looping construct in Go. It may be used with a single condition, which is equivalent to a while statement, or the condition can be omitted, which is an endless loop. Parenthesis are not required:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;for i := 1; i &lt;= int(orgID); i++ { ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A &lt;code&gt;for&lt;/code&gt; statement can also iterate through the entries of an array, slice, string, or map:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;for _, team := range query.Result { ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The blank identifier &lt;code&gt;_&lt;/code&gt; is an anonymous placeholder which indicates that the first value returned by range, the index, is ignored. Go permits break and continue to specify a label. The label must refer to a &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;switch&lt;/code&gt;, or &lt;code&gt;select&lt;/code&gt; statement.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;LOOP: for { ... break LOOP }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In a &lt;code&gt;switch&lt;/code&gt; statement, case labels do not fall through. You can make them fall through using the fallthrough keyword. A case may have multiple values. The case value need not be an integer; it can be any type that supports equality comparisons.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;switch hs.Cfg.Protocol { case setting.HTTPScheme, setting.SocketScheme: ... default: }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The increment and decrement operators may only be used in statements, not in expressions.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;patchedIndex++&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go has pointers but not pointer arithmetic. You cannot use a pointer variable to walk through the bytes of a string. Go uses &lt;code&gt;nil&lt;/code&gt; for invalid pointers, where C++ uses &lt;code&gt;NULL&lt;/code&gt; or simply &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;b := &amp;bytes.Buffer{} copy := *msg&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Functions&lt;/h2&gt; &lt;p&gt;A function in Go is defined with the &lt;code&gt;func&lt;/code&gt; keyword. Input and output parameters are defined separately. There can be multiple return types. In the following example, &lt;code&gt;name&lt;/code&gt; is an input string parameter. The function returns two strings.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;func splitName(name string) (string, string) { names := util.SplitString(name) switch len(names) { case 0: return" ","" case 1: return names[0], "" default: return names[0], names[1] } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The return values can be returned by name. For example, in the above:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;func splitName(name string) (s1 string, s2 string) { ... s1 = names[0] s2 = names[1] ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Parameters are passed by value except for maps and slices, which are passed by reference. The &lt;code&gt;defer&lt;/code&gt; statement can be used to call a function after the function containing the &lt;code&gt;defer&lt;/code&gt; statement returns.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;func performGet(url string, av *Avatar, handler ResponseHandler) error { ... defer func() { if err := resp.Body.Close(); err != nil { alog.Warn("Failed to close response body", "err", err) }() ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is typically used to handle cleanup, such as closing files before the containing function returns. A function definition without a corresponding function name is an anonymous function, as shown in the previous example. The anonymous function can reference variables in the containing function's scope.&lt;/p&gt; &lt;p&gt;Each variable in Go exists as long as there are references to the variable. Go uses garbage collection to free the variable's memory when there are no longer references to it. The memory cannot be released explicitly. The garbage collection is intended to be incremental and efficient on modern processors.&lt;/p&gt; &lt;h2&gt;Interfaces&lt;/h2&gt; &lt;p&gt;Go uses interfaces in situations where C++ uses classes, subclasses, and templates. A Go interface is similar to a C++ pure abstract class: a class with pure virtual methods and no data members. Go allows any type that provides the methods named in the interface to be treated as an implementation of the interface.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;type Expander interface { SetupExpander(file *ini.File) error Expand(string) (string, error) }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A method definition is similar to a function definition with the addition of a receiver, which is similar to the &lt;code&gt;this&lt;/code&gt; pointer in a C++ class method.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;type fileExpander struct { } func (e fileExpander) SetupExpander(file *ini.File) error { return nil } func (e fileExpander) Expand(s string) (string, error) { ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;fileExpander&lt;/code&gt; implements the Expander interface by defining a &lt;code&gt;SetupExpander&lt;/code&gt; and &lt;code&gt;Expand&lt;/code&gt; method. Any function that takes &lt;code&gt;Expander&lt;/code&gt; as a parameter will accept a variable of type &lt;code&gt;fileExpander&lt;/code&gt;. If we think of &lt;code&gt;fileExpander&lt;/code&gt; as a C++ pure abstract base class, then defining &lt;code&gt;SetupExpander&lt;/code&gt; and &lt;code&gt;Expand&lt;/code&gt; for &lt;code&gt;fileExpander&lt;/code&gt; made it inherit from &lt;code&gt;Expander&lt;/code&gt;. A type may satisfy multiple interfaces.&lt;/p&gt; &lt;p&gt;An anonymous field can be used to implement something resembling a C++ child class:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; type myExpanderType struct { Expander; count int }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This implements &lt;code&gt;myExpanderType&lt;/code&gt; as a child of &lt;code&gt;Expander&lt;/code&gt; that inherits its methods.&lt;/p&gt; &lt;p&gt;A variable that has an interface type may be converted to have a different interface type using a special construct called a type assertion. This is implemented dynamically at run time, like C++ dynamic cast. Unlike dynamic cast, there does not need to be any declared relationship between the two interfaces. This is typically used in a type switch, which switches based on the type of value:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; switch v := value.(type) { case time.Time: return v.Format(timeFormat) case error: return v.Error() case fmt.Stringer: return v.String() default: return v }&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Goroutines&lt;/h3&gt; &lt;p&gt;Go permits starting a new thread of execution, known as a goroutine, using the &lt;code&gt;go&lt;/code&gt; statement. The function runs in a different, newly created goroutine, which shares the address space with the parent. The Go runtime schedules an arbitrary number of goroutines onto a random number of OS threads. Goroutines are more lightweight than OS threads, use a smaller stack, and are scheduled by a userspace runtime scheduler.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; func updateUsageStats(ctx context.Context, reader *bluge.Reader, logger log.Logger) { ... } go updateUsageStats(context.Background(), reader, i.logger)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go statements frequently use function literals:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; go func() { defer close(done) for { ... }()&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Channels&lt;/h2&gt; &lt;p&gt;Channels are used to communicate between goroutines. Any value may be sent over a channel. Channels are efficient and cheap. To send a value on a channel, use &lt;code&gt;&lt;-&lt;/code&gt; as a binary operator. To receive a value on a channel, use &lt;code&gt;&lt;-&lt;/code&gt; as a unary operator. When calling functions, channels are passed by reference. A channel can control access to a single value.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; attemptChan := make(chan int, 1) attemptChan &lt;- 1 for { ... go e.processJob(attemptID, attemptChan, cancelChan, job) select { case &lt;-unfinishedWorkTimer.C: return e.endJob(grafanaCtx.Err(), cancelChan, job) case &lt;-attemptChan: return e.endJob(nil, cancelChan, job) }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Development environment&lt;/h2&gt; &lt;p&gt;Go provides dependency management, interface abstraction, and documentation tools to assist with programming in the large.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;go&lt;/code&gt; command provides various tools for managing the Go development environment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;go help&lt;/code&gt; gives an overview of the &lt;code&gt;go&lt;/code&gt; command. &lt;/li&gt; &lt;li&gt;&lt;code&gt;go env&lt;/code&gt; displays the environment variables that define the Go environment.&lt;/li&gt; &lt;li&gt;The Go compiler and tools are installed into the value displayed by GOROOT.&lt;/li&gt; &lt;li&gt;&lt;code&gt;gofmt&lt;/code&gt; is a tool that enforces layout rules. Most Go code has been run through &lt;code&gt;gofmt&lt;/code&gt;. It enforces a single standard Go style.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Go does not use header files. Instead, each source file is a member of a set of related files that form a module. Go has tools to manage versions of modules. Executables created by go are typically statically linked; thus the referenced modules are statically linked.&lt;/p&gt; &lt;p&gt;When a package defines an object (type, constant, variable, function) with a name starting with an upper case letter, that object is visible to any other file that imports that package. Exported names and functions form a module application binary interface, which should not be changed or removed within a major release so that compatibility is maintained.&lt;/p&gt; &lt;p&gt;The environment variable &lt;code&gt;GOPATH&lt;/code&gt;, which is typically &lt;code&gt;$HOME/go&lt;/code&gt; and can be displayed by &lt;code&gt;go env&lt;/code&gt;, contains a directory: &lt;code&gt;pkg&lt;/code&gt; where compiled package files are stored based on import statements. Go downloads modules into the directory &lt;code&gt;$GOMODCACHE&lt;/code&gt;; the default path is &lt;code&gt;pkg/mod&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;go mod init&lt;/code&gt; creates a new &lt;code&gt;go.mod&lt;/code&gt; file, which defines the module contents in the current directory. If after creating &lt;code&gt;go.mod&lt;/code&gt; we create a simple Go program:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;package main import ( "fmt" ) func main() { var s string = "abc" ss := "def" var ssa = []string {"efg","hij"} fmt.Println(s, ss, ssa) }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;go mod init example.com/test&lt;/code&gt; creates a &lt;code&gt;go.mod&lt;/code&gt; that describes a module's characteristics. &lt;/p&gt; &lt;p&gt;&lt;code&gt;go mod edit&lt;/code&gt; modifies the attributes in &lt;code&gt;go.mod&lt;/code&gt;. &lt;/p&gt; &lt;p&gt;&lt;code&gt;go mod tidy&lt;/code&gt; will download any modules that our module is importing.&lt;/p&gt; &lt;p&gt;&lt;code&gt;go run .&lt;/code&gt; will build and execute a Go program. &lt;/p&gt; &lt;p&gt;&lt;code&gt;go build .&lt;/code&gt; will build a Go program&lt;/p&gt; &lt;p&gt;&lt;code&gt;./test&lt;/code&gt; abc def [efg hij]&lt;/p&gt; &lt;p&gt;Go provides a test infrastructure. To add a module test, create a program &lt;code&gt;MOD_test.go&lt;/code&gt;, where &lt;code&gt;MOD&lt;/code&gt; is the module name. The program imports "testing." Each test routine is called Test where Testfoo is an individual test name. &lt;code&gt;t.Fatalf&lt;/code&gt; is called if the test fails.&lt;/p&gt; &lt;pre&gt; package MOD import { "testing" } func Testfoo(t *testing.T) { ... if ... { t.Fatal(`Expected %d but got %d, error`, expected, value) } } &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Go is an easy-to-learn language with a large ecosystem of tools. This article gives a C++ programmer the fundamentals to start using Go effectively.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/30/go-c-developers-beginners-guide" title="Go for C++ developers: A beginner's guide"&gt;Go for C++ developers: A beginner's guide&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Stan Cox</dc:creator><dc:date>2023-08-30T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.4.5 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-4-5" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-4-5</id><updated>2023-08-30T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.4.5 has just been released. It fixes a few bugs that have been reported by the community</content><dc:creator>Julien Viet</dc:creator></entry><entry><title>Conquer CORS errors in OpenShift web applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/29/avoid-cors-errors-openshift-web-applications" /><author><name>Evan Shortiss</name></author><id>f6dcaacf-6295-4d9e-a103-5d15a1d0fdee</id><updated>2023-08-29T07:00:00Z</updated><published>2023-08-29T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, I'll give a brief overview of &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS"&gt;cross-origin resource sharing (CORS)&lt;/a&gt; in the context of modern web applications and their interactions with HTTP APIs. I will also provide a &lt;a href="https://github.com/evanshortiss/cors-nginx-blogpost"&gt;sample solution(hosted on GitHub)&lt;/a&gt; for avoiding CORS-related issues in a web application that's served from &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The sample solution uses &lt;a href="https://catalog.redhat.com/software/base-images"&gt;Red Hat's Universal Base Images&lt;/a&gt; for &lt;a href="https://catalog.redhat.com/software/containers/search?q=nodejs&amp;p=1&amp;vendor_name=Red%20Hat"&gt;Node.js&lt;/a&gt;, &lt;a href="https://catalog.redhat.com/software/containers/search?q=nginx&amp;p=1&amp;vendor_name=Red%20Hat"&gt;NGINX&lt;/a&gt;, and &lt;a href="https://catalog.redhat.com/software/containers/search?q=openjdk&amp;p=1&amp;vendor_name=Red%20Hat"&gt;Java&lt;/a&gt;. Instructions are included in this post to help you deploy and test the sample solution for free in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What exactly is CORS?&lt;/h2&gt; &lt;p&gt;If you're not familiar with CORS or need a refresher, you should take a look at this excellent primer published by &lt;a href="https://wizardzines.com/comics/cors/"&gt;Julia Evans on wizardzines.com&lt;/a&gt;. To summarize, when &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; executing in a web browser on one origin (e.g., &lt;code&gt;https://ui.foo.bar&lt;/code&gt;) initiates a request to an endpoint running on another origin (e.g., &lt;code&gt;https://api.foo.bar&lt;/code&gt;), it's classed as a &lt;strong&gt;cross-origin request&lt;/strong&gt;. If the target origin of the request doesn't return the appropriate &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#the_http_response_headers"&gt;CORS headers&lt;/a&gt; to confirm that it supports cross-origin requests, then the browser returns an error to the JavaScript code that invoked the request instead of revealing the contents of the response. In some instances, cross-origin requests also require a &lt;a href="https://developer.mozilla.org/en-US/docs/Glossary/Preflight_request"&gt;preflight HTTP OPTIONS request&lt;/a&gt;, which adds overhead to your application!&lt;/p&gt; &lt;p&gt;You might be wondering why web browsers enforce CORS security measures. Imagine a scenario where you visit a malicious website, and it executes JavaScript that initiates a cross-origin request to your bank's servers. If you had recently visited your bank's website, your web browser might still have a valid session stored, meaning the malicious request could potentially succeed in accessing your banking information. Thankfully, web browsers enforce CORS security measures to prevent this from happening.&lt;/p&gt; &lt;h2&gt;CORS in practice&lt;/h2&gt; &lt;p&gt;Needless to say, there are legitimate reasons for an application running on one origin to request resources from another origin; otherwise you wouldn't be reading this article. For example, a common scenario is that a web application hosted on an origin such as &lt;code&gt;ui.foo.bar&lt;/code&gt; needs to request resources from a HTTP API hosted on &lt;code&gt;api.foo.bar&lt;/code&gt;. Figure 1 illustrates this using an NGINX web server for the static content and a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt;-based &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; application serving up an HTTP API.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cors-before.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cors-before.png?itok=hdj2hHFk" width="600" height="214" alt="A web browser that is requesting static assets from ui.foo.bar, and requesting resources from an HTTP API hosted at api.foo.bar" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: A web application communicating with an API using cross-origin requests.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In Figure 1, the requests from the React web application (&lt;code&gt;ui.foo.bar&lt;/code&gt;) to the Quarkus-based HTTP API (&lt;code&gt;api.foo.bar&lt;/code&gt;) will only be successful if the appropriate &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#the_http_response_headers"&gt;CORS headers&lt;/a&gt; are set on the responses returned by the HTTP API. This also assumes that &lt;code&gt;api.foo.bar&lt;/code&gt; is hosted at a publicly addressable endpoint and that the team building it is willing to configure it to support CORS.&lt;/p&gt; &lt;p&gt;Assuming there's no way to return the CORS headers from the HTTP API, a common workaround is to use a CORS proxy, as shown in Figure 2. The CORS proxy will set the appropriate CORS headers, e.g., &lt;code&gt;Access-Control-Allow-Origin: ui.foo.bar&lt;/code&gt; before returning responses from the HTTP API to the web browser.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cors-proxy.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cors-proxy.png?itok=0ownhKXb" width="600" height="371" alt="Using a CORS Proxy to Inject Access-Control Headers" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Using a CORS proxy to inject access-control headers to avoid CORS errors.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A third solution, and the one I will demonstrate in this post, doesn't require setting any CORS headers and doesn't require a CORS proxy. That's because this solution will result in all requests being classified as same-origin, eliminating CORS entirely! It's possible to implement this solution using &lt;a href="https://docs.openshift.com/container-platform/4.13/networking/routes/route-configuration.html#nw-path-based-routes_route-configuration"&gt;path-based routing built into OpenShift Routes&lt;/a&gt;, or by using the web server (such as Apache or NGINX) that's used to serve the React application as a &lt;a href="https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/"&gt;reverse proxy&lt;/a&gt; to access the HTTP API.&lt;/p&gt; &lt;h2&gt;NGINX reverse proxy solution&lt;/h2&gt; &lt;p&gt;A sample solution is fully documented in &lt;a href="https://github.com/evanshortiss/cors-nginx-blogpost"&gt;this repository on GitHub&lt;/a&gt;; I'll provide an overview of the key points here, though. Figure 3 illustrates this solution.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cors-after.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cors-after.png?itok=Yp_cW-gn" width="600" height="315" alt="Using the same NGINX web server that hosts the frontend as reverse proxy to reach a HTTP API." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Using the NGINX to serve static assets and as a reverse proxy to access an HTTP API.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The real magic is in the NGINX configuration in the &lt;a href="https://github.com/evanshortiss/cors-nginx-blogpost/blob/main/react-ui/nginx.conf#L50-L80"&gt;react-ui/nginx.conf&lt;/a&gt; file. NGINX serves up the React application, but it also defines a custom location block that proxies HTTP API requests to the Quarkus back end:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apacheconf"&gt;location /hello { resolver $NAMESERVERS valid=10s; proxy_pass "$API_URL$request_uri"; proxy_redirect "$API_URL" ""; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration might vary slightly from application to application, but this particular location block instructs NGINX to do the following for all request paths beginning with &lt;code&gt;/hello&lt;/code&gt;:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Use the nameservers defined in &lt;code&gt;$NAMESERVERS&lt;/code&gt; to resolve hostnames. This is important because the sample solution uses an internal OpenShift service URL, not a public hostname for the back end.&lt;/li&gt; &lt;li&gt;Proxy any request beginning with &lt;code&gt;/hello&lt;/code&gt; to an upstream URL defined by the &lt;code&gt;$API_URL&lt;/code&gt; variable.&lt;/li&gt; &lt;li&gt;Remove the internal hostname from the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Location"&gt;Location response header&lt;/a&gt; returned by the upstream API server. This ensures that if the API server issues a redirect, the URL for the redirect is pointing at NGINX and not the internal hostname for the Pod.&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can find more information related to the proxy directives in &lt;a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html"&gt; NGINX's official ngx_http_proxy_module documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;API_URL&lt;/code&gt; and &lt;code&gt;NAMESERVERS&lt;/code&gt; variables referenced in the &lt;code&gt;nginx.conf&lt;/code&gt; are replaced when the NGINX container starts, as seen in the &lt;a href="https://github.com/evanshortiss/cors-nginx-blogpost/blob/main/react-ui/Containerfile#L28"&gt;CMD in the Containerfile&lt;/a&gt; for the image. As you might expect, the &lt;code&gt;API_URL&lt;/code&gt; is the base URL of the Quarkus HTTP API, and the &lt;code&gt;NAMESERVERS&lt;/code&gt; are specified to ensure that NGINX can resolve the internal &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service&lt;/a&gt; hostname targeted by &lt;strong&gt;API_URL&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As for the React application, the code used to interact with the HTTP API can perform a same-origin request without specifying the hostname:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const response = await fetch('/hello')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's pretty convenient, right? Since the React application can issue same-origin requests to reach the Quarkus HTTP API, there's no need to worry about CORS.&lt;/p&gt; &lt;p&gt;The following section will guide you through deploying the application to try it out for yourself, and with it deployed you'll be able to explore the &lt;a href="https://docs.openshift.com/container-platform/4.13/networking/routes/route-configuration.html#nw-path-based-routes_route-configuration"&gt;path-based routing&lt;/a&gt; solution using OpenShift Routes.&lt;/p&gt; &lt;h2&gt;Try it out&lt;/h2&gt; &lt;p&gt;You can deploy the first sample solution on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt; Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and experiment with it for free. Follow these steps to deploy the NGINX reverse proxy solution, then continue reading to learn about the path-based solution.&lt;/p&gt; &lt;p&gt;Log in to &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox&lt;/a&gt;, then open a terminal using the Web Terminal icon ( &lt;strong&gt;&gt;_ &lt;/strong&gt;) in the upper-right corner. Issue the following commands in the terminal to clone the source repository and deploy the React-based front end and Quarkus-based back end into your project:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/evanshortiss/cors-nginx-blogpost cors-blog cd cors-blog export NAMESPACE=$(oc project -q) export SOURCE_REPOSITORY="https://github.com/evanshortiss/cors-nginx-blogpost" oc process -f manifests/frontend.yaml -p NAMESPACE=$NAMESPACE -p SOURCE_REPOSITORY=$SOURCE_REPOSITORY | oc apply -f - oc process -f manifests/backend.yaml -p NAMESPACE=$NAMESPACE -p SOURCE_REPOSITORY=$SOURCE_REPOSITORY | oc apply -f -&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Confirm that both applications are up and running using Topology View from the Developer perspective, as shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-08-04_at_11.53.52_am_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-08-04_at_11.53.52_am_0.png?itok=RvyhVz_b" width="600" height="338" alt="The OpenShift Topology View showing NGINX and Quarkus Services running in a Project." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: NGINX and Quarkus deployed on OpenShift as part of the sample solution.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Click the arrow icon in the top-right of the NGINX item in the &lt;strong&gt;Topology View&lt;/strong&gt; to open the React application in your web browser. The application has two buttons. The first performs a same-origin request to the Quarkus back end via NGINX, while the second performs a cross-origin request directly to the Quarkus backend.&lt;/p&gt; &lt;p&gt;As you might expect, the same-origin request routed via NGINX will succeed, but the cross-origin request will fail because the Quarkus HTTP API isn't configured to return the necessary CORS headers in its responses. You can use the Developer Tools in your web browser to confirm this, as shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-08-04_at_12.01.39_pm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-08-04_at_12.01.39_pm.png?itok=zS8s0FXG" width="600" height="337" alt="Viewing a CORS Error in Chrome DevTools" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Viewing a successful same-origin request and a failed cross-origin request in Chrome's Developer Tools.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;OpenShift path-based routing solution&lt;/h2&gt; &lt;p&gt;This solution is less flexible and only works if your back end is hosted on OpenShift. The upside is that it doesn't require you to maintain an NGINX proxy configuration. As part of this solution, you'll create two &lt;a href="https://docs.openshift.com/container-platform/4.13/networking/routes/route-configuration.html#nw-path-based-routes_route-configuration"&gt;OpenShift routes&lt;/a&gt; that share a hostname. The first route will serve the HTML, CSS, and JavaScript by routing traffic to NGINX. The second route will be configured to use the same hostname as the first, but only route traffic that matches a specific URL pattern directly to the Quarkus HTTP API. Figure 5 illustrates the solution that uses OpenShift path-based Routes.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cors-path-based.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cors-path-based.png?itok=WBIKHNol" width="600" height="318" alt="Two OpenShift Routes using a shared hostname to implement path-based routing." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Implementing path-based routing using two OpenShift routes that share a hostname.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Start by deploying the sample application per the instructions in the last section. Next, you'll create a route that directs traffic to the NGINX service that serves the React application. Open the web terminal and create a file named &lt;code&gt;ui-route.yaml&lt;/code&gt; that contains the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: Route apiVersion: route.openshift.io/v1 metadata: name: example spec: to: kind: Service name: react-ui tls: termination: edge insecureEdgeTerminationPolicy: None path: / port: targetPort: 8080-tcp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to create a route to access the React application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create -f ui-route.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Obtain the hostname for your new route using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get route example -o jsonpath='{.spec.host}' | xargs echo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a second file named &lt;code&gt;backend-route.yaml&lt;/code&gt; that contains the following content. Make sure you replace the &lt;code&gt;$HOSTNAME&lt;/code&gt; placeholder to match the value returned from the &lt;code&gt;oc get route&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: Route apiVersion: route.openshift.io/v1 metadata: name: example-backend spec: # Note that adding a trailing slash here will require incoming # requests to include the trailing slash to match this route. # Omit the trailing slash if both /hello and /hello/ should match. path: /hello # Replace $HOSTNAME with the value you obtained with "oc get route" host: $HOSTNAME to: kind: Service name: quarkus-backend weight: 100 port: targetPort: 8080 tls: termination: edge wildcardPolicy: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This route specification uses the same hostname as the route you created to access the React application, but it only matches URLs starting with &lt;code&gt;/hello&lt;/code&gt; and routes them to the Quarkus back end. Create the second route by issuing the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create -f backend-route.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can test your new path-based configuration is working by visiting the URL returned by this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get route example -o jsonpath='https://{.spec.host}' | xargs echo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Visit the URL, enter a message in the text input, and then use the &lt;strong&gt;same-origin&lt;/strong&gt; button to make a request. Confirm that the request was routed directly to the Quarkus back end using OpenShifts's path-based routing by viewing the Quarkus pod's logs and inspecting the &lt;code&gt;X-Forwarded-For&lt;/code&gt; headers using these commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export POD_NAME=$(oc get pods -l 'app=quarkus-backend' -o jsonpath={.items[0].metadata.name}) oc logs $POD_NAME | grep -i 'x-forwarded-for'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The logs should contain an entry with two IP addresses listed in the &lt;strong&gt;X-Forwarded-For&lt;/strong&gt; header and a more recent entry with a single IP address in the &lt;strong&gt;X-Forwarded-For&lt;/strong&gt; header. The entry that was routed using path-based routing contains just your IP address. The entries routed via NGINX contain both your IP address and the IP address of an &lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index"&gt;OpenShift Router&lt;/a&gt; Pod because the previous implementation involves the NGINX forwarding the requests to Quarkus application on behalf of the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index"&gt;OpenShift Router&lt;/a&gt;. This can be seen in Figure 6.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-08-04_at_5.06.04_pm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-08-04_at_5.06.04_pm.png?itok=2lsch3cL" width="600" height="337" alt="Logs from a Quarkus HTTP API Pod showing the differences between the X-Forwarded-For header values when using path-based routing versus the NGINX reverse proxy." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Logs from a Quarkus HTTP API pod showing the differences between the X-Forwarded-For header values when using path-based routing versus the NGINX reverse proxy.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Dealing with CORS can be tedious, but now you're equipped with the knowledge to work through it. Consider using the techniques discussed here with your applications hosted on OpenShift to avoid CORS entirely. The most important thing is never to get lazy and return &lt;code&gt;Access-Control-Allow-Origin: *&lt;/code&gt; from your services unless you're 100% sure that it's the right decision for your application.&lt;/p&gt; &lt;p&gt;And, of course, be sure to try using the solutions outlined in this article in the &lt;a href="https://developers.redhat.com/developer-sandbox/"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/29/avoid-cors-errors-openshift-web-applications" title="Conquer CORS errors in OpenShift web applications"&gt;Conquer CORS errors in OpenShift web applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2023-08-29T07:00:00Z</dc:date></entry></feed>
